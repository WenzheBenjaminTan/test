---
layout: post
title: "支持向量机" 
---
# 1、支持向量机基本模型

给定训练样本集$$D = \{(\boldsymbol{x}_1,y_1), (\boldsymbol{x}_2,y_2),..., (\boldsymbol{x}_m,y_m)\}, y_i\in\{1,-1\}$$，分类学习最基本的想法就是基于训练集$D$在样本空间中找到一个分类超平面（separating hyperplane），将不同类别的样本分开。

在样本空间中，分类超平面可通过如下线性方程来描述：

$$\boldsymbol{w}^T\boldsymbol{x} + b = 0$$

其中，$$\boldsymbol{w} = (w_1;w_2;...;w_d)$$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。显然，分类超平面可由法向量$\boldsymbol{w}$和位移$b$完全确定，我们可以将分类超平面记为$(\boldsymbol{w},b)$。

样本空间中任意点$\boldsymbol{x}'$到超平面$(\boldsymbol{w},b)$的距离为：

$$r = \frac{|\boldsymbol{w}^T\boldsymbol{x}' + b|}{\|\boldsymbol{w}\|}$$

假设超平面$$(\boldsymbol{w}',b')$$能够将训练样本正确分类，则总是可以通过缩放变换得到：

$$\begin{equation}\label{1}\begin{cases}\boldsymbol{w}^T\boldsymbol{x}_i + b \geq +1 & y_i = 1 \\
\boldsymbol{w}^T\boldsymbol{x}_i + b \leq -1 & y_i = -1
\end{cases}\end{equation}$$

使得式\eqref{1}中等号成立的训练样本被称为“支持向量”（support vector），两个异类支持向量到分类超平面的距离之和称为该分类超平面的“间隔”（margin），表示为：

$$\gamma = \frac{2}{\|\boldsymbol{w}\|}$$

欲找到具有“最大间隔”（maximum margin）的分类超平面，也就是要找到能满足式\eqref{1}中约束的参数$\boldsymbol{w}$和$b$，使得$\gamma$最大，即：

$$
\begin{align}
	&\max_{\boldsymbol{w},b} \frac{2}{\|\boldsymbol{w}\|} \\
	s.t.\ & y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)\geq 1 \ \   i=1,2,...,m 
\end{align}
$$

显然，最大化间隔，仅需要最大化$$\|\boldsymbol{w}\|^{-1}$$，这等价于最小化$$\|\boldsymbol{w}\|^{2}$$。于是，上面的优化模型可重写为：

$$
\begin{align}
	&\min_{\boldsymbol{w},b} \frac{1}{2}\|\boldsymbol{w}\|^2 \\
	s.t.\ & y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)\geq 1, \ \   i=1,2,...,m 
\end{align}
$$

这就是支持向量机（Support Vector Machine，简称SVM）的基本模型，它也是“判别式模型”。


# 2、基本模型的求解

支持向量机的基本模型本身是一个凸二次规划（convex quadratic programming）问题，能直接用现成的优化计算包求解，但我们可以有更高效的求解方法。

对模型中每条约束添加拉格朗日乘子$\alpha_i \geq 0$，则该问题的拉格朗日函数可写为：

$$\begin{equation}\label{2}L(\boldsymbol{w},b,\boldsymbol{\alpha}) = \frac{1}{2}\|\boldsymbol{w}\|^2 + \sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b))
\end{equation}$$

其中$$\boldsymbol{\alpha} = (\alpha_1; \alpha_2; ...; \alpha_m)$$。

令$$L(\boldsymbol{w},b,\boldsymbol{\alpha})$$对$\boldsymbol{w}$和$b$的偏导分别为零，可得：

$$\boldsymbol{w} = \sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i$$

$$\sum_{i=1}^m\alpha_iy_i = 0$$

进而可得基本模型的拉格朗日对偶问题为：

$$
\begin{align}
	&\max_{\boldsymbol{\alpha}} \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j{\boldsymbol{x}_i}^T\boldsymbol{x}_j \\
	s.t.\ & \sum_{i=1}^m\alpha_iy_i = 0\\
		& \alpha_i \geq 0, \ \   i=1,2,...,m 
\end{align}
$$

解出$\boldsymbol{\alpha}$后，即可求出$\boldsymbol{w}$，进而利用支持向量的性质求出偏移项$b$：对任意支持向量$$(\boldsymbol{x}_s,y_s)$$都有

$$y_s(\sum_{i=1}^m\alpha_iy_i{\boldsymbol{x}_i}^T\boldsymbol{x} + b) = 1$$

于是可求得：

$$b = \frac{1}{y_s} - \sum_{i=1}^m\alpha_iy_i{\boldsymbol{x}_i}^T\boldsymbol{x}$$

理论上可通过选取任意支持向量并通过求解上式获得$b$，但现实任务中常采用更鲁棒的做法：使用所有支持向量求解的平均值。

于是最终得到学习到的分类模型：

$$f(\boldsymbol{x}) = \sum_{i=1}^m\alpha_iy_i{\boldsymbol{x}_i}^T\boldsymbol{x} + b$$

基本模型是个凸优化问题，且满足Slater's condition，因此强对偶性成立，对偶问题解出的$\boldsymbol{\alpha}$满足KKT（Karush-Kuhn-Tucker）条件，于是容易得到：

$$\begin{cases}\alpha_i\geq 0 \\
y_if(\boldsymbol{x}_i) - 1 \geq 0 \\
\alpha_i(y_if(\boldsymbol{x}_i) - 1) = 0
\end{cases}$$

于是，对于任意训练样本$(\boldsymbol{x}_i,y_i)$，总有$\alpha_i= 0$或$$y_if(\boldsymbol{x}_i) = 1$$。如果是$\alpha_i= 0$，则该样本对$f(\boldsymbol{x})$不会有任何影响；如果是$$y_if(\boldsymbol{x}_i) = 1$$，则该样本位于最大间隔边界上，是一个支持向量。这体现了支持向量机的一个重要性质：训练完成后，最终模型仅与支持向量有关，其余样本都可以抛弃。


# 3、核函数

在基本模型中，我们假设训练样本是线性可分的，然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面。

对于这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。理论上，如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。

令$\boldsymbol{\phi}(\boldsymbol{x})$表示将样本$\boldsymbol{x}$映射后的特征向量，于是，在特征空间中分类超平面对应的模型可表示为：

$$\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}) + b = 0$$

其中$\boldsymbol{w}$和$b$为模型参数。

类似地，支持向量机原问题为：

$$
\begin{align}
	&\min_{\boldsymbol{w},b} \frac{1}{2}\|\boldsymbol{w}\|^2 \\
	s.t.\ & y_i(\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}_i) + b)\geq 1, \ \   i=1,2,...,m 
\end{align}
$$

其对偶问题为：

$$
\begin{align}
	&\max_{\boldsymbol{\alpha}} \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j{\boldsymbol{\phi}(\boldsymbol{x}_i)}^T\boldsymbol{\phi}(\boldsymbol{x}_j) \label{dual_general} \\
	s.t.\ & \sum_{i=1}^m\alpha_iy_i = 0\\
		& \alpha_i \geq 0, \ \   i=1,2,...,m 
\end{align}
$$

求解式\eqref{dual_general}涉及到计算$${\boldsymbol{\phi}(\boldsymbol{x}_i)}^T\boldsymbol{\phi}(\boldsymbol{x}_j)$$，这是样本$\boldsymbol{x}_i$与$\boldsymbol{x}_j$映射到特征空间之后的内积。由于特征空间的维数可能很高，甚至可能是无穷维，因此直接计算$${\boldsymbol{\phi}(\boldsymbol{x}_i)}^T\boldsymbol{\phi}(\boldsymbol{x}_j)$$通常是困难的。为了避开这个障碍，可以设想这样一个函数：

$$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j) = {\boldsymbol{\phi}(\boldsymbol{x}_i)}^T\boldsymbol{\phi}(\boldsymbol{x}_j)$$

即样本$\boldsymbol{x}_i$与$\boldsymbol{x}_j$映射到特征空间之后的内积等于它们在原始空间中通过函数$\kappa$计算的结果。有了这样的函数，我们就不必直接去计算高维甚至无穷维特征空间中的内积，于是对偶问题可以重写为：

$$
\begin{align}
	&\max_{\boldsymbol{\alpha}} \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)  \\
	s.t.\ & \sum_{i=1}^m\alpha_iy_i = 0\\
		& \alpha_i \geq 0, \ \   i=1,2,...,m 
\end{align}
$$

求解$\boldsymbol{\alpha}$后可得到最终学习到的分类模型为：

$$f(\boldsymbol{x}) = \sum_{i=1}^m\alpha_iy_i\kappa(\boldsymbol{x},\boldsymbol{x}_i) + b$$

这里的函数$\kappa$就是“核函数”（kernel function）。可以从上式看出，模型最优解可通过训练样本的核函数展开，这一展开式被称作“支持向量展式”（support vector expansion）。

显然，若已知特征映射$\boldsymbol{\phi}$的具体形式，则可很容易地写出核函数$\kappa$。但在现实任务中，我们通常不知道$$\boldsymbol{\phi}$$是什么形式，那么合适的核函数是否一定存在？什么样的函数能做核函数呢？

**定理** 令$\chi$为样本输入空间，$\kappa$是定义在$\chi\times\chi$上的对称函数，则$\kappa$是核函数当且仅当对于任意数据$$D = \{\boldsymbol{x}_1, \boldsymbol{x}_2, ..., \boldsymbol{x}_m\}$$，“核矩阵”（kernel matrix）$\mathbf{K}$总是半正定的：

$$\mathbf{K} = \begin{bmatrix} 
\kappa(\boldsymbol{x}_1,\boldsymbol{x}_1) & \cdots & \kappa(\boldsymbol{x}_1,\boldsymbol{x}_j) & \cdots & \kappa(\boldsymbol{x}_1,\boldsymbol{x}_m) \\
\vdots & \ddots & \vdots & \ddots & \vdots \\
\kappa(\boldsymbol{x}_i,\boldsymbol{x}_1) & \cdots & \kappa(\boldsymbol{x}_i,\boldsymbol{x}_j) & \cdots & \kappa(\boldsymbol{x}_i,\boldsymbol{x}_m) \\
\vdots & \ddots & \vdots & \ddots & \vdots \\
\kappa(\boldsymbol{x}_m,\boldsymbol{x}_1) & \cdots & \kappa(\boldsymbol{x}_m,\boldsymbol{x}_j) & \cdots & \kappa(\boldsymbol{x}_m,\boldsymbol{x}_m)
\end{bmatrix}$$

事实上，对于一个半正定核矩阵，总能找到一个与之对应的特征映射$\boldsymbol{\phi}$。换言之，任何一个核函数都隐式地定义了一个称为“再生核希尔伯特空间”（Reproducing Kernel Hilbert Space，简称RKHS）的特征空间。

尽管我们希望样本在特征空间中是线性可分的，但在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的（核函数仅是隐式地定义了这个特征空间）。于是，“核函数选择”称为支持向量机的最大变数。若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。

# 4、软间隔

现实任务中往往很难确定合适的核函数使得训练样本在特征空间中绝对线性可分；退一步说，即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果是不是由于过度拟合所造成的。

缓解该问题的办法是允许支持向量机在一些样本上出错。为此，要引入“软间隔”（soft margin）的概念。具体来说，前面介绍的支持向量机要求所有样本均满足约束条件，即所有样本都必须分类正确，这称为“硬间隔”（hard margin），而软间隔则是允许某些样本不满足约束条件。

当然，在最大化间隔的同时，不满足约束条件的情况应尽量少。于是，优化目标可写为：

$$\begin{equation}\min_{\boldsymbol{w},b} \frac{1}{2}\|\boldsymbol{w}\|^2 + C \sum_{i=1}^{m}\ell_{\text{hinge}}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b))\end{equation}$$

其中$C > 0$是个常数，$$\ell_{\text{hinge}}$$是“hinge损失函数”：

$$\ell_{\text{hinge}}(z) = \max(0,1-z)$$

因此，优化目标为：

$$\begin{equation}\label{hinge}\min_{\boldsymbol{w},b} \frac{1}{2}\|\boldsymbol{w}\|^2 + C \sum_{i=1}^{m}\max(0, 1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b))\end{equation}$$

引入“松弛变量”（slack variables）$\xi_i\geq 0$，可将式\eqref{hinge}重写为：

$$
\begin{align}
	&\min_{\boldsymbol{w},b,\boldsymbol{\xi}} \frac{1}{2}\|\boldsymbol{w}\|^2 + C\sum_{i=1}^m\xi_i\\
	s.t.\ & y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)\geq 1 - \xi_i, \\
	& \xi_i\geq 0 \ \   i=1,2,...,m 
\end{align}
$$

这就是常用的“软间隔支持向量机”。

这仍然是一个凸二次规划问题，通过拉格朗日乘子法可得拉格朗日函数为：

$$\begin{equation}L(\boldsymbol{w},b,\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\mu}) = \frac{1}{2}\|\boldsymbol{w}\|^2 + C\sum_{i=1}^m\xi_i + \sum_{i=1}^m\alpha_i(1-\xi_i-y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)) - \sum_{i=1}^m\mu_i\xi_i
\end{equation}$$

其中，$$\alpha_i\geq 0$$、$$\mu_i\geq 0$$是拉格朗日乘子。

令$$L(\boldsymbol{w},b,\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\mu})$$对$\boldsymbol{w}$、$b$和$$\xi_i$$的偏导分别为零，可得：

$$\boldsymbol{w} = \sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i$$

$$\sum_{i=1}^m\alpha_iy_i = 0$$

$$\begin{equation}\label{am}\alpha_i + \mu_i = C \end{equation}$$

进而可得对偶问题：

$$
\begin{align}
	&\max_{\boldsymbol{\alpha}} \sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j{\boldsymbol{x}_i}^T\boldsymbol{x}_j \label{soft_dual}\\
	s.t.\ & \sum_{i=1}^m\alpha_iy_i = 0\\
		& 0\leq \alpha_i \leq C, \ \   i=1,2,...,m 
\end{align}
$$

可以看出软间隔与硬间隔的对偶问题唯一的差别就在对偶变量的约束不同，前者是$$0\leq \alpha_i \leq C$$，后者是$$0\leq \alpha_i$$。求解后能得到同样地支持向量展式，引入核函数后也一样。

因为强对偶性仍然满足，对软间隔支持向量机，KKT条件要求：

$$\begin{cases}\alpha_i\geq 0,\ \mu_i\geq 0 \\
y_if(\boldsymbol{x}_i) - 1 + \xi_i \geq 0 \\
\alpha_i(y_if(\boldsymbol{x}_i) - 1 + \xi_i) = 0 \\
\xi_i\geq 0,\ \mu_i\xi_i = 0
\end{cases}$$

于是，对于任意训练样本$(\boldsymbol{x}_i,y_i)$，总有$\alpha_i= 0$或$$y_if(\boldsymbol{x}_i) = 1 - \xi_i$$。如果是$\alpha_i= 0$，则该样本对$f(\boldsymbol{x})$不会有任何影响；如果是$$y_if(\boldsymbol{x}_i) = 1 - \xi_i$$，则该样本仍然被称为是一个支持向量：由式\eqref{am}可知，若$$\alpha_i < C$$则$$\mu_i > 0$$，进而$\xi_i = 0$，即该样本正好在最大间隔边界上；若$$\alpha_i = C$$，则$$\mu_i = 0$$，此时若$$\xi_i \leq 1$$则该样本落在最大间隔内部，若$$\xi_i > 1$$则该样本被错误分类。因此，软间隔支持向量机的最终分类模型也仅与支持向量有关，即通过采用hinge损失函数仍保持了稀疏性。

# 5、正则化

我们还可以使用其他损失函数来代替“软间隔”优化目标中的hinge损失函数，以得到其他学习模型。但这些学习模型都具有一个共性：优化目标中的第一项用来描述分类超平面的“间隔”大小，第二项用来表述训练集上的误差。因此，可以写为更一般的形式：

$$\begin{equation} \label{reg} \min_{f} \Omega(f) + C \sum_{i=1}^{m}\ell(f(\boldsymbol{x}_i),y_i)\end{equation}$$

其中$f$是要学习得到的模型；$\Omega(f)$ 称为“结构风险”（structural risk），用于描述模型$f$的某些性质；$$\sum_{i=1}^{m}\ell(f(\boldsymbol{x}_i),y_i)$$称为“经验风险”（empirical risk），用于描述模型与训练数据的契合程度；$C$用于对二者进行折中。

从结构风险最小化的角度来看，$\Omega(f)$表述了我们希望获得具有何种性质的模型（例如希望获得权重参数较小的模型），这为引入领域知识和用户意图提供了途径。另一方面，该信息有利于削减模型的假设空间，从而降低了最小化训练误差的过拟合风险，从这个角度来说，式\eqref{reg}称为“正则化”（regularization）问题，$\Omega(f)$称为正则化项，$C$则称为正则化常数。$$\mathrm{L}_p$$范数（norm）是常用的正则化项，其中$$\mathrm{L}_2$$范数$$\|\boldsymbol{w}\|_2$$倾向于$$\boldsymbol{w}$$的分量取值尽量均衡，$$\mathrm{L}_0$$范数$$\|\boldsymbol{w}\|_0$$和$$\mathrm{L}_1$$范数$$\|\boldsymbol{w}\|_1$$则倾向于于$$\boldsymbol{w}$$的分量尽量稀疏，即非零分量个数尽量少。

# 6、支持向量回归

现在我们来考虑回归问题。给定训练样本集$$D = \{(\boldsymbol{x}_1,y_1), (\boldsymbol{x}_2,y_2),..., (\boldsymbol{x}_m,y_m)\}, y_i\in \mathbb{R}$$，希望学得一个形式如下的回归模型：

$$\begin{equation} \label{rem} f(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} + b \end{equation}$$

使得$f(\boldsymbol{x})$与$y$尽可能接近，$\boldsymbol{w}$和$b$是待确定的模型参数。

对样本$(\boldsymbol{x},y)$，传统回归模型通常直接基于模型输出$f(\boldsymbol{x})$与真实输出$y$之间的差别来计算损失函数值，当且仅当$f(\boldsymbol{x})$与$y$完全相同时，损失才为零。不同的是，支持向量回归（Support Vector Regression，简称SVR）假设我们能容忍$f(\boldsymbol{x})$与$y$之间最多有$\epsilon$的偏差，即仅当$f(\boldsymbol{x})$与$y$之间的差别绝对值大于$\epsilon$时才计算损失。这就相当于以$f(\boldsymbol{x})$为中心，构建了一个宽度为$2\epsilon$的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。

于是，SVR问题可形式化为：

$$\begin{equation}\label{svr}\min_{\boldsymbol{w},b} \frac{1}{2}\|\boldsymbol{w}\|^2 + C \sum_{i=1}^{m}\ell_{\epsilon}(f(\boldsymbol{x}_i)-y_i)\end{equation}$$

其中$C$为正则化常数，$$\ell_{\epsilon}$$为$$\epsilon$$-insensitive损失函数：

$$\ell_{\epsilon}(z) = \begin{cases}0 & \text{if } |z|\leq\epsilon \\ |z|-\epsilon & \text{otherwise}\end{cases}$$


引入“松弛变量”（slack variables）$$\xi_i\geq 0$$ 和$$\widehat{\xi}_i\geq 0$$ ，可将式\eqref{svr}重写为：

$$
\begin{align}
	&\min_{\boldsymbol{w},b,\boldsymbol{\xi}} \frac{1}{2}\|\boldsymbol{w}\|^2 + C\sum_{i=1}^m(\xi_i+\widehat{\xi}_i)\\
	s.t.\ & f(\boldsymbol{x}_i)-y_i \leq \epsilon + \xi_i, \\
	& y_i - f(\boldsymbol{x}_i) \leq \epsilon + \widehat{\xi}_i, \\
	& \xi_i\geq 0, \widehat{\xi}_i\geq 0 \ \   i=1,2,...,m 
\end{align}
$$

这仍然是一个凸二次规划问题，通过拉格朗日乘子法可得拉格朗日函数为：

$$\begin{equation}L(\boldsymbol{w},b,\boldsymbol{\xi},\boldsymbol{\widehat{\xi}},\boldsymbol{\alpha},\boldsymbol{\widehat{\alpha}},\boldsymbol{\mu},\boldsymbol{\widehat{\mu}}) = \\
\frac{1}{2}\|\boldsymbol{w}\|^2 + C\sum_{i=1}^m(\xi_i+\widehat{\xi}_i) + \sum_{i=1}^m\alpha_i(f(\boldsymbol{x}_i)-y_i - \epsilon - \xi_i) + \sum_{i=1}^m\widehat{\alpha}_i(y_i - f(\boldsymbol{x}_i) - \epsilon - \widehat{\xi}_i) - \sum_{i=1}^m\mu_i\xi_i - \sum_{i=1}^m\widehat{\mu}_i\widehat{\xi}_i
\end{equation}$$

其中，$$\alpha_i\geq 0$$、$$\widehat{\alpha}_i\geq 0$$、$$\mu_i\geq 0$$、$$\widehat{\mu}_i\geq 0$$是拉格朗日乘子。

将式\eqref{rem}代入，再令$$L(\boldsymbol{w},b,\boldsymbol{\xi},\boldsymbol{\widehat{\xi}},\boldsymbol{\alpha},\boldsymbol{\widehat{\alpha}},\boldsymbol{\mu},\boldsymbol{\widehat{\mu}})$$对$\boldsymbol{w}$、$b$、$$\xi_i$$和$$\widehat{\xi}_i$$的偏导分别为零可得：

$$\begin{equation}\label{wvalue}\boldsymbol{w} = \sum_{i=1}^m(\widehat{\alpha}_i-\alpha_i)\boldsymbol{x}_i\end{equation}$$

$$\sum_{i=1}^m(\widehat{\alpha}_i-\alpha_i) = 0$$

$$\begin{equation}\alpha_i + \mu_i = C \end{equation}$$

$$\begin{equation}\widehat{\alpha}_i + \widehat{\mu}_i = C \end{equation}$$

进而可得对偶问题：

$$
\begin{align}
	&\max_{\boldsymbol{\alpha},\boldsymbol{\widehat{\alpha}}} \sum_{i=1}^m(y_i(\widehat{\alpha}_i-\alpha_i)-\epsilon(\widehat{\alpha}_i+\alpha_i)) - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m(\widehat{\alpha}_i-\alpha_i)(\widehat{\alpha}_j-\alpha_j){\boldsymbol{x}_i}^T\boldsymbol{x}_j \\
	s.t.\ & \sum_{i=1}^m(\widehat{\alpha}_i-\alpha_i) = 0\\
		& 0\leq \alpha_i,\widehat{\alpha}_i \leq C, \ \   i=1,2,...,m 
\end{align}
$$

因为强对偶性仍然满足，KKT条件成立：

$$\begin{cases}\alpha_i(f(\boldsymbol{x}_i)-y_i - \epsilon - \xi_i) = 0 \\
\widehat{\alpha}_i(y_i - f(\boldsymbol{x}_i) - \epsilon - \widehat{\xi}_i) = 0 \\
\alpha_i\widehat{\alpha}_i = 0,\ \xi_i\widehat{\xi}_i = 0 \\
(C-\alpha_i)\xi_i = 0, \ (C-\widehat{\alpha}_i)\widehat{\xi}_i = 0
\end{cases}$$

可以看出，当且仅当$$f(\boldsymbol{x}_i)-y_i - \epsilon - \xi_i = 0$$时，$$\alpha_i$$能取非零值，当且仅当$$y_i - f(\boldsymbol{x}_i) - \epsilon - \widehat{\xi}_i = 0$$时，$$\widehat{\alpha}_i$$能取非零值。换言之，仅当样本$$(\boldsymbol{x}_i,y_i)$$不落入$$\epsilon$$-间隔带中，相应的$$\alpha_i$$和$$\widehat{\alpha}_i$$才能取非零值（落在$$\epsilon$$-间隔带中的样本都满足$$\alpha_i = \widehat{\alpha}_i = 0$$）。此外，显然约束$$f(\boldsymbol{x}_i)-y_i - \epsilon - \xi_i = 0$$和$$y_i - f(\boldsymbol{x}_i) - \epsilon - \widehat{\xi}_i = 0$$不能同时成立，因此$$\alpha_i$$和$$\widehat{\alpha}_i$$至少有一个为零。

将式\eqref{wvalue}代入式\eqref{rem}，可得SVR的解形如：

$$\begin{equation} \label{svr_so} f(\boldsymbol{x}) = \sum_{i=1}^m(\widehat{\alpha}_i-\alpha_i){\boldsymbol{x}_i}^T\boldsymbol{x} + b \end{equation}$$

能使式\eqref{svr_so}中的$$\widehat{\alpha}_i-\alpha_i\neq 0$$的样本即为SVR的支持向量，它们必不在$$\epsilon$$-间隔带之内。显然，SVR的支持向量仅是训练样本的一部分，即其解仍然具有稀疏性。

由KKT条件可以看出，对于每个样本$$(\boldsymbol{x}_i,y_i)$$都有$$(C-\alpha_i)\xi_i = 0$$且$$\alpha_i(f(\boldsymbol{x}_i)-y_i - \epsilon - \xi_i) = 0$$。于是，在得到$$\alpha_i$$后，若$$0 < \alpha_i < C$$，则必有$$\xi_i = 0$$（即样本正好在$$\epsilon$$-间隔带边缘上），进而有：

$$b = y_i + \epsilon - \sum_{i=1}^m(\widehat{\alpha}_i-\alpha_i){\boldsymbol{x}_i}^T\boldsymbol{x}$$

因此，再求得对偶问题的解后，理论上可任意选取满足$$0 < \alpha_i < C$$的样本通过上式求得$b$。实践中，常采用一种更鲁棒的方法：选取多个或所有满足条件$$0 < \alpha_i < C$$的样本求解$b$后取平均值。

若考虑特征映射形式，则SVR的解可表示为：

$$f(\boldsymbol{x}) = \sum_{i=1}^m(\widehat{\alpha}_i-\alpha_i)\kappa(\boldsymbol{x},\boldsymbol{x}_i) +b$$

其中，$$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j) = {\boldsymbol{\phi}(\boldsymbol{x}_i)}^T\boldsymbol{\phi}(\boldsymbol{x}_j)$$为核函数。

因此，给定训练样本$$\{(\boldsymbol{x}_1,y_1), (\boldsymbol{x}_2,y_2),..., (\boldsymbol{x}_m,y_m)\}$$，若不考虑偏移项$b$，则无论SVM还是SVR，学习得到的模型总能表示成核函数$$\kappa(\boldsymbol{x}_i,\boldsymbol{x})$$的线性组合。
