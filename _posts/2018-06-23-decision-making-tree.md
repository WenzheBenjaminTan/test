---
layout: post
title: "决策树" 
---

# 1、引言

决策树是一种实现分治策略的层次数据结构，它是一种有效的“判别式模型”，可以用于分类和回归。

决策树由一些内部决策节点（decision node）和终端树叶节点（leaf node）组成。每个决策节点$m$实现一个具有离散输出的测试函数$$f_m(\boldsymbol{x})$$，用于标记分支。给定一个输入在决策节点应用测试，并根据测试输出确定下一个分支。这一过程从根节点开始，并递归地重复，直至到达一个树叶节点。这时，树叶节点的值形成输出。

每个决策节点的$$f_m(\boldsymbol{x})$$定义了一个输入空间的判别式，不同的决策树方法对$$f_m(\boldsymbol{x})$$假设不同的模型。每一个树叶节点有一个输出标签：对于分类，该标签是类代码；而对于回归，该标签是一个数值。一个树叶节点定义了输入空间的一个局部区域，落入该区域的输入实例具有相同的输出。区域的边界被从树根到该树叶的路径上的内部决策节点中的判别式定义。

决策树的层次安排使得输入实例的区域可以快速确定。比如说，如果决策是二元的，则在最好情况下每个决策可以去掉一半的备选区域。假设总共有$b$个区域，则在最好情况下可以平均通过$$\log_2b$$次决策找到正确的区域。决策树的另一个优点是可解释性：可以很容易地把决策树转换称一组容易理解的IF-THEN规则。

我们将从一个决策节点只使用一个输入变量的单变量树开始，介绍如何为分类和回归构造这样的树。然后，我们将这种方法推广到多变量树。

# 2、单变量树

在单变量树（univariate tree）中，每个内部决策节点的测试只使用一个输入维$$x_i$$。如果输入维$$x_i$$是非数值（有序）的，取$V$个可能的值之一，则决策节点检查$$x_i$$的值，并取相应的分支，实现一个$V$路划分。如果输入维$$x_i$$是数值（有序）的，则决策节点的测试是比较：

$$f_m(\boldsymbol{x}): x_i \geq w_{m0}$$

其中$$w_{m0}$$是适当选择的阈值。该决策节点将将输入空间一分为二：$$L_m = \{\boldsymbol{x}\mid x_i \geq w_{m0}\}$$ 和 $$R_m = \{\boldsymbol{x}\mid x_i < w_{m0}\}$$。称作二元划分（binary split）（此时$V=2$）。

构造给定训练样本的树的过程叫作树学习。对于给定的训练集，存在许多对它进行无错编码的树，而为了简单起见，我们感兴趣的是寻找其中最小的树。这里树的大小用树中的节点数和决策节点判别式模型的复杂性来度量。寻找最小树问题是NP-完全的，我们必须使用基于启发式的局部搜索方法，在合理的时间内得到合理的树。

决策树学习的基本算法如下：

输入：训练集$$D=\{(\boldsymbol{x}^{(1)},y^{(1)}), (\boldsymbol{x}^{(2)},y^{(2)}), ..., (\boldsymbol{x}^{(N)},y^{(N)})\}$$；属性集$$A=\{a_1,a_2,...,a_d\}$$。

过程：函数$$TreeGenerate(D,A)$$

1：生成节点node；

2：if $D$中样本全属于同一类别$C$ then

3：&emsp; 将node标记为$C$类叶节点；return

4：end if

5：if $A = \varnothing$ or $D$中样本在$A$上取值完全相同 then

6：&emsp; 将node标记为叶节点，其类别标记为$D$中样本数最多的类；return

7：end if

8：从$A$中选择最优划分属性$$a_*$$；

9：for $$a_*$$的每一个值$$a_*^v$$ do

10：&emsp; 为node生成一个分支，令$D^v$表示$D$中在$$a_*$$上取值为$$a_*^v$$的样本子集；

11：&emsp; if $D^v = \varnothing$ then 

12：&emsp; &emsp; 将分支节点标记为叶节点，其类别标记为$D$中样本最多的类；return

13：&emsp; else

14：&emsp; &emsp; 以$$TreeGenerate(D^v,A\backslash \{a_*\})$$为分支节点；

15：&emsp; end if

16：end for

输出：完整的决策树


显然，决策树生成是一个贪心递归过程。其中有以下三种情形会导致递归返回：(1) 当前节点包含的样本全属于同一类别，无需划分；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；(3) 当前节点包含的样本集合为空，不能划分。

在第(2)种情形下，我们把当前节点标记为叶节点，并将其类别设定为该节点所包含样本最多的类别；在第(3)种情形下，同样把当前节点标记为叶节点，但将其类别设定为其父节点包含样本最多的类别。注意这两种情形的处理实质不同：情形(2)是在利用当前节点的后验分布，而情形(3)则是把父节点的样本分布作为当前节点的先验分布。

## 2.1 分类树

决策树学习的关键是算法的第8行，即如何选择最优划分属性。在用于分类的树，即分类树（classification tree）中，划分的优劣用不纯性量度（impurity measure）来定量分析。如果划分后对应的实例都属于相同的类，则称该划分是纯的。

### 2.1.1 信息收益准则

“信息熵”（information entropy）也是度量样本集合不纯性的一种常用指标。同样假设到达节点$m$的样本集合为$$D_m$$，其中属于$C_i$类的样本比例为$p_m^i$，$$D_m$$的信息熵定义为：

$$Ent(D_m) = -\sum_{i=1}^Kp_m^i \log p_m^i$$

其中定义$$0\log 0 \equiv 0$$。在信息论中，熵是对一个实例的类代码进行编码所需要的最少位数。对于二类问题，如果$$p^1=1$$，$$p^0=0$$，则所有的实例都属于$$C_1$$类，我们什么都不需要发送，熵为0。如果$$p^1=p^0=0.5$$，则我们需要发送一位数来通告两种情况之一，即熵为1。在这两个极端之间，我们可以设计编码，更可能的类用较短的编码，更不可能的类用较长的编码，可以让平均下来每个信息使用不足一位。当存在$K > 2$时，形同的讨论成立，并且当$p^i=1/K$时熵最大，为$\log K$。

对于任意属性$a$，如果它是非数值型的，则有$V$个可能的输出，而如果它是数值型的，则只有两个可能输出（即$V=2$）。若使用$a$来对样本集$$D_m$$进行划分，则会产生$V$个分支节点，其中第$v$个分支节点包含的样本集合记为$$D_m^v$$。于是我们可以计算每个分支节点的信息熵，并根据样本比例进行加权求和，可以得到该属性划分后的总熵值，并进而得到用属性$a$对节点$m$进行划分所获得的“信息收益”（information gain）

$$Gain(D_m, a) = Ent(D_m) -\sum_{v=1}^V\frac{\mid D_m^v\mid}{\mid D_m\mid}Ent(D_m^v)$$

对于数值属性，为了得到$$D_m^v$$，我们还需要知道节点$m$的划分阈值$$w_{m0}$$。在$$\mid D_m^v\mid$$个数据点之间存在$$\mid D_m^v\mid-1$$个可能的$$w_{m0}$$：我们不需要考虑所有（无限个）可能的点；例如，我们只需要考虑两个样本点之间的中值就够了。还有一点要注意，最佳划分阈值总是在属于两个不同类的两个相邻样本点之间。这样我们可以从中选取使得该属性划分获得的信息收益最大的划分阈值。

ID3决策树学习算法就是以信息收益为准则来选择划分属性的，即在上述算法第8行选择属性：

$$a_* = arg\max_{a\in A} Gain(D,a)$$

### 2.1.2 收益率准则

实际上，信息收益准则对可取值数目较多的属性有所偏好，为了减少这种偏好可能带来的不利影响，C4.5决策树学习算法不直接使用信息收益，而是使用“收益率”（gain ratio）来选择最优划分属性。

收益率定义为：

$$GainRatio(D_m,a) = \frac{Gain(D_m,a)}{IV_m(a)}$$

其中

$$IV_m(a) = -\sum_{v=1}^V\frac{\mid D_m^v\mid}{\mid D_m\mid}\log \frac{\mid D_m^v\mid}{\mid D_m\mid}$$

称为属性$a$的“固有值”（instrinsic value）。属性$a$的可能取值数目越多（即$V$越大），则$$IV_m(a)$$通常也会越大。

需要注意的是，收益率准则对可取值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择收益率最大的候选划分属性，而是使用一个启发式的方法：先从候选划分属性中找出信息收益高于平均水平的属性，再从中选择收益率最高的。

### 2.1.3 基尼指数准则

CART（Classification And Regression Tree）决策树学习算法使用“基尼系数”（Gini index）来衡量样本集合的不纯性。$$D_m$$的基尼值定义为：

$$Gini(D_m) = \sum_{i=1}^K\sum_{j\neq i} p_m^ip_m^j$$

直观来说，$$Gini(D_m)$$反映了从$$D_m$$中随机抽取两个样本，其类别标记不一致的概率。

属性$a$的基尼指数定义为：

$$GiniIndex(D_m,a) = \sum_{v=1}^V\frac{\mid D_m^v\mid}{\mid D_m\mid}Gini(D_m^v)$$

于是我们在上述算法第8行选择的最优属性为：

$$a_* = arg\min_{a\in A} GiniIndex(D,a)$$




## 2.2 回归树

回归树（regression tree）可以用几乎与分类树完全相同的方法构造，唯一的不同是适合分类的不纯性度量用适合回归的不纯性度量取代。

设$b_m(\boldsymbol{x})=1$表示实例$\boldsymbol{x}$到达节点$m$，否则为0。在回归树中，划分的好坏用估计值的均方误差来度量。令$$g_m$$为节点$m$的估计值，则均方误差表示为

$$E_m = \frac{1}{N_m}\sum_i(g_m-y^{(i)})^2b_m(\boldsymbol{x}^{(i)})$$

其中$$N_m = \sum_ib_m(\boldsymbol{x}^{(i)})$$。节点的估计值我们一般使用到达该节点的所有实例的输出均值（如果噪声太大用中值）

$$g_m = \frac{\sum_ib_m(\boldsymbol{x}^{(i)})y^{(i)}}{\sum_ib_m(\boldsymbol{x}^{(i)})}$$

于是均方误差对应于节点上的方差。如果在一个节点上，误差是可以接受的，即$$E_m < \theta_r$$，则创建一个树叶节点，存放$$g_m$$值。如果误差不能接受，则对到达节点$m$的实例进一步划分，使得诸分支的误差和最小。与分类一样，在每个节点上，我们寻找最小误差的属性（以及数值属性的划分阈值），然后递归地进行上述过程。

# 3、剪枝

通常，如果到达一个节点的训练实例数小于训练集的某个百分比（如5%），则无论是否不纯或是否有错误，该节点都不进一步分裂。其基于的思想是：基于较少实例的决策树导致较大方差，从而导致较大泛化误差。在树构造之前停止构造称作树的预剪枝（prepruning）。

得到较小树的另一种做法是后剪枝（postpruning）。前面我们看到，树的学习过程中很贪心，在每一步，我们做出一个决策（即产生一个决策节点）并继续进行，绝不回溯尝试其他可能的选择。而后剪枝则可以，我们可以试图找出并剪掉不必要的子树。

在后剪枝中，我们让树完全增长直到所有树叶都是纯的并具有零训练误差，然后我们找出导致过拟合的子树并剪掉它们。最开始时，我们从被标记的数据集中保留一个剪枝集（pruning set），在训练阶段不使用。对于每棵子树，我们用一个被该子树覆盖的训练实例标记的树叶节点替换它。如果该树叶在剪枝集上的性能不比该子树差，则剪掉该子树并保留树叶节点，因为子树附加的复杂性是不必要的；否则保留子树。

预剪枝与后剪枝相比，预剪枝较快，但是后剪枝通常导致更准确的树。

# 4、多变量树

在构造单变量树时，只使用一个输入维来划分。而在构造多变量树（multivariate tree）时，在每个决策节点都可以使用所有的输入维，因此更加一般。在学习过程中不是为每个非叶子节点寻找一个最优划分属性，而是试图建立一个合适的分类器（确定一组参数）。

当所有输入都是数值属性时，线性多变量节点定义为

$$f_m(\boldsymbol{x}): \boldsymbol{w}^T\boldsymbol{x}+w_{m0} > 0$$

还可以使用非线性多变量节点

$$f_m(\boldsymbol{x}): \boldsymbol{x}^T\boldsymbol{W}_m\boldsymbol{x}+\boldsymbol{w}^T\boldsymbol{x}+w_{m0} > 0$$

还有一种可能是使用球形节点

$$f_m(\boldsymbol{x}): \|\boldsymbol{x} - \boldsymbol{c}_m\| \leq \alpha_m$$

其中$$\boldsymbol{c}_m$$是圆心，$$\alpha_m$$是半径。




