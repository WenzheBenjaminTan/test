---
layout: post
title: "非参数学习" 
---

# 1、参数学习与非参数学习

在参数学习中，无论是密度估计、分类还是回归，我们都假设了一个在输入空间上有效的模型。于是，我们可以把估计概率密度函数、判别式函数或回归函数问题都归结为估计少量参数值。它的缺点是，假设并非总是成立的，并且不成立时可能会导致很大的误差。

在非参数学习（nonparametric learning）中，我们只假定相似的输入具有相似的输出。这是一种合理的假设：无论是概率密度函数、判别式函数还是回归函数都是平滑、缓慢地变化的，相似的实例意味着相似的事物。

这样，我们的算法需要使用合适的距离度量，从训练集中找出相近的实例，并且基于它们来进行推断和计算。

在参数学习方法中，所有训练实例都影响最终的全局估计；而在非参数学习方法中，不一定存在全局模型，一般情况下，每次估计时采用局部模型，即只受邻近实例的影响。

非参数学习又称为基于实例（instance-based）或基于记忆（memory-based）的学习，因为它们所做的一般是把训练实例存放在一个查找表中备用。这意味着所有训练实例都要存放，而存放所有训练实例需要$O(N)$存储量。此外，给定一个输入，应当找出相似的训练实例，而找出它们需要$O(N)$计算量。这种方法也称惰性（lazy）学习算法，因为不像急切（eager）的参数学习，给定训练集时，它们并不计算模型，而是将计算推迟到给定一个检验实例时才进行。对于参数学习方法，模型都比较简单，一般具有$O(d)$或$O(d^2)$量级个参数，并且一旦从训练集中计算出这些参数，我们保存模型并且计算输出时也不再需要训练集，计算量也为$O(d)$或$O(d^2)$。通常，$N$比$d$（或$d^2$）大得多，这种存储量和计算量的增加是非参数学习的缺点。


# 2、非参数密度估计

与通常的密度估计一样，我们假设样本$\mathcal{X} = \\{x^t\\}_{t=1}^N$独立地从一个未知的概率密度$p(\cdot)$中抽取的，$\widehat{p}(\cdot)$是$p(\cdot)$的估计。我们从单变量情况开始，即$x^t$是标量，稍后我们将推广到多维的情况。

累积分布函数$F(x)$的非参数估计是小于或等于$x$的样本所占的比例：

$$\widehat{F}(x) = \frac{\#\{x^t \leq x\}}{N}$$

其中$$\#\{x^t \leq x\}$$表示$x^t$小于或等于$x$的训练样本数。

类似地，密度函数的非参数估计可以表示为：

$$\widehat{p}(x) = \frac{1}{h}\left(\frac{\#\{x^t \leq x+h\} - \#\{x^t \leq x\}}{N}\right)$$

其中$h$是区间长度，并且假定落入该区间的实例$x^t$是“足够接近”的。

## 2.1 直方图估计

最古老、最流行的方法是直方图估计（hisogram estimator）。在直方图中，输入空间被划分为称作“箱”（bin）的相等区间。给定原点$x_0$和箱宽$h$，箱区间为$$[x_0+mh, x_0+(m+1)h]$$（$m$是整数），密度估计由下式给出：

$$\widehat{p}(x) = \frac{\#\{x^t\text{ in the same bin as }x\}}{Nh}$$

直方图估计的优点是一旦计算和存放了箱估计，我们就不再需要保留训练集了。

质朴估计法（maive estimator）使得我们不必设置原点（但此时我们需要保留整个训练集）。它定义为：

$$\widehat{p}(x) = \frac{\#\{x-h/2 < x^t \leq x+h/2\}}{Nh}$$

容易看出，它等于$x$落在宽度为$h$的箱中心的直方图估计。

该估计还可以表示为：

$$\widehat{p}(x) = \frac{1}{Nh}\sum_{t=1}^N w(\frac{x-x^t}{h})$$

其中权重函数定义为：

$$w(u) = \begin{cases} 1 & -1/2<u\leq 1/2 \\
			0 & otherwise\end{cases}$$

这就好像对每个$x^t$都有一个围绕它的大小为$h$的影响区域，并且对落入该区域的$x$产生影响，而非参数估计恰为影响区域包含$x$的$x^t$的影响之和。该估计不是连续函数，并在$x^t\pm h/2$处有跳跃。

## 2.2 核估计

为了得到光滑的估计，我们使用一个光滑的权重函数，称作核函数（kernel function）。最流行的是高斯核：

$$K(u) = \frac{1}{\sqrt{2\pi}}\exp(-\frac{u^2}{2})$$

核估计（kernel estimator）定义为：

$$\widehat{p}(x) = \frac{1}{Nh}\sum_{t=1}^N K(\frac{x-x^t}{h})$$

此时所有的$x^t$都对$x$上的估计有影响，并且其影响随着$\mid x^t-x\mid$的增加而平滑地减小。

当$h$很小时，每个训练实例只在其附近小区域内产生较大影响，而在较远的点上影响非常小。当$h$较大时，有更多的核重叠，我们得到较光滑的估计。如果$K(\cdot)$处处非负并且积分为1，即如果它是合法的密度函数，则$\widehat{p}(\cdot)$也是。

核估计的一个问题是窗口宽度在整个输入空间上是固定的。不过已经有些文献提出各种自适应方法将$h$看作$x$附近密度的函数。

## 2.3 k-最近邻估计

