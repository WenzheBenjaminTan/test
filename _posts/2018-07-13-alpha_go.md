---
layout: post
title: "AlphaGo算法框架" 
---

# 1、蒙特卡洛树搜索（Monte Carlo Tree Search，MCTS）

蒙特卡洛树搜索是一种在强化学习问题中做出最优决策的方法，主要用于各种动态博弈中的行动策略规划，最终会生成一棵完全展开的博弈树。在博弈树中，每个节点表示的是一个状态$s$，同时还包含两个重要信息：一个是根据模拟结果估计的值$v_s$，另一个是该节点已经被访问的次数$n_s$；每一条边表示的是在某一状态下采取的动作$a$。

蒙特卡洛树搜索的主要概念是搜索，即沿着博弈树向下的一组遍历过程。单次遍历过程如下：

1）选择（Selection）：从博弈树的根结点（当前的博弈状态）按照一定的选择规则延伸到一个没有完全展开（即有未被访问的子节点）的节点L；

2）扩展（Expansion）：如果节点L不是终止节点（即不会导致博弈游戏终止），则选择一个未访问的子节点C作为模拟的起始点；

3）模拟（Simulation）：从节点C开始运行一个模拟，即从起始博弈状态开始，逐步采用一个rollout策略函数进行动作选择，直到博弈游戏终止得到输出结果，当然，也可以直接使用一个评估模型来评估节点C得到输出结果；

4）反向传播（Back Propagation）：将节点C标注为已访问，并将模拟的结果反向传播回当前博弈树的根结点，更新传播过程中每个节点的统计数据（$v_s$和$n_s$）。

在对博弈树向下遍历时，节点选择通过选择最大化某个量来实现。可以使用MCTS的一个特例——上线置信区间方法（Upper Confidence Bounds for Trees，UCT）来说明。

在UCT方法中，使用UCB（Upper Confidence Bound）函数，该函数给出了在当前节点$s$，其子节点$s_i$被选择的可能性：

$$UCB(s,s_i) = \frac{v_{s_i}}{n_{s_i}} + c\sqrt{\frac{\log(n_s)}{n_{s_i}}}$$

其中$c$是可调整参数。UCB函数具有利用（exploitation）和探索（exploration）的思想，其中第一个组件$\frac{v_{s_i}}{n_{s_i}}$是exploitation组件，可以理解为总模拟奖励（simulation reward）除以总访问次数，即节点$s_i$的胜率评估结果。第二个组件$\sqrt{\frac{\log(n_s)}{n_{s_i}}}$是exploration组件，它支持访问未被探索的节点，这些节点相对来说更少被访问（$n_{s_i}$较小）。而参数$c$则用于控制蒙特卡洛树搜索中explitation和exploration组件之间的权衡。

下面给出UCT方法流程：

1）从博弈树的根结点开始根据UCB函数向下搜索；

2）假设遇到节点s，如果节点s存在从未访问过的子节点则执行步骤3，否则执行步骤4；

3）扩展未访问的子节点并进行模拟评估，得到结果后更新该子节点至根节点路径上所有节点的估计值和访问次数，执行步骤1；

4）计算每个子节点的UCB值，将UCB值最高的子节点作为节点s，执行步骤2；

5）算法可随时终止，通常是达到给定时间或访问总次数。

MCTS最终选择的是具有最多访问计数的节点所对应的动作。

# 2、算法框架

AlphaGo完整的学习系统主要由以下四个部分组成：

1）策略网络（policy network）。又分为监督学习的策略网络$p_{\boldsymbol{\sigma}}$和强化学习的策略网络$p_{\boldsymbol{\rho}}$。策略网络的作用是根据当前的棋局来预测和采样下一步走棋。

2）滚轮策略（rollout policy）$p_{\boldsymbol{\pi}}$。也是用于预测和采样下一步走棋，但是预测速度是监督学习策略网络的1500倍。

3）估值网络（value network）$v_{\boldsymbol{\theta}}(s)$。用于估计当前棋局的价值。

4）MCTS。将策略网络、滚轮策略和估值网络融合进策略搜索的过程中，形成一个完整的走棋系统。

策略网络、滚轮策略的输入是当前棋局，输出是下步每一种走棋的概率；而估值网络的输入是当前棋局，输出是当前棋局的价值。

具体实施分为四个阶段：

在第一阶段，使用策略网络$p_{\boldsymbol{\sigma}}$来直接对来自人类专家下棋的样本数据进行学习。$p_{\boldsymbol{\sigma}}$是一个13层的深度卷积网络，具体的训练方式为梯度下降法：

$$\Delta\boldsymbol{\sigma} \propto \frac{\partial \log p_{\boldsymbol{\sigma}}(a\mid s)}{\partial\boldsymbol{\sigma}}$$

在测试集上使用所有输入特征进行训练，预测人类专家走子动作的准确率为57.0%。同时，还使用局部特征训练了一个可以迅速走子采样的滚轮策略$p_{\boldsymbol{\pi}}$，预测人类专家走子动作的准确率为24.2%。$p_{\boldsymbol{\sigma}}$每下一步棋是3毫秒，而$p_{\boldsymbol{\pi}}$是2微秒。

在第二阶段，使用$p_{\boldsymbol{\sigma}}$作为输入，通过强化学习的策略梯度方法来训练策略网络$p_{\boldsymbol{\rho}}$：

$$\Delta\boldsymbol{\rho} \propto \frac{\partial \log p_{\boldsymbol{\rho}}(a\mid s)}{\partial\boldsymbol{\rho}}z$$

其中$z$表示一局棋最终所获的收益，胜为+1，负为-1，平0。

具体训练方式是：随机选择之前迭代轮的策略网络和当前的策略网络进行对弈，并利用策略梯度法（该方法属于REINFORCE算法）来更新参数，最终得到增强的策略网络$p_{\boldsymbol{\rho}}$。$p_{\boldsymbol{\rho}}$与$p_{\boldsymbol{\sigma}}$在结构上是完全相同的。增强后的$p_{\boldsymbol{\rho}}$与$p_{\boldsymbol{\sigma}}$对抗时胜率超过了80%。

在第三阶段，主要关注的是对当前棋局的价值评估。具体通过最小化估值网络输出$v_{\boldsymbol{\theta}}(s)$和收益$z$（通过增强后的$p_{\boldsymbol{\rho}}$自我对弈得到）之间的均方误差来训练估值网络（该方法属于蒙特卡洛方法）：

$$\Delta\boldsymbol{\theta} \propto \frac{\partial v_{\boldsymbol{\theta}}(s)}{\partial\boldsymbol{\theta}}(z-v_{\boldsymbol{\theta}}(s))$$

估值网络采用的结构与策略网络类似。

在第四阶段，主要基于策略网络、滚轮策略和估值网络进行蒙特卡洛树搜索。

每个博弈树的边$(s,a)$存储着三个重要量：状态-动作值$Q(s,a)$、访问计数$N(s,a)$和先验概率$P(s,a)$。

在每个时间步$t$，从状态$s_t$中选择一个走子动作$a_t$：

$$a_t = arg\max_a(Q(s_t,a)+u(s_t,a))$$

其中，$u(s_t,a)$表示额外的奖励，目的是鼓励探索前提下最大化走子动作的值：

$$u(s,a) \propto \frac{P(s,a)}{1+N(s,a)}$$

其中，$P(s,a) = p_{\boldsymbol{\rho}}(a\mid s)$，即用策略网络的输出作为先验概率。$u(s,a)$与先验概率成正比，与访问计数成反比。

当遍历$L$步到达一个叶节点$s_L$时，综合估值网络输出$v_{\boldsymbol{\theta}}(s_L)$和使用滚轮策略$p_{\boldsymbol{\pi}}$的模拟结果$z_L$来获得叶子节点的值：

$$V(s_l) = (1-\lambda)v_{\boldsymbol{\theta}}(s_L) + \lambda z_L$$

边$(s,a)$的访问计数和对应的值计算方式如下：

$$N(s,a) = \sum_{i=1}^N 1(s,a,i)$$

$$Q(s,a) = \frac{\sum_{i=1}^N 1(s,a,i)V(s_L^i)}{N(s,a)}$$

其中，$1(s,a,i)$与状态动作对$(s,a)$是否在第$i$次模拟中被访问有关，是为1，否为0；$s_L^i$表示第$i$次模拟时的叶子节点。

一旦完成了搜索过程，则Agent走子时从训练好的博弈树的根结点位置选择访问计数最多的走子动作。之前搜索过的数据可能仍然在当前考虑的博弈树范围内，这就可以重复使用数据而不是从头建新的博弈树，这可以减少MCTS的时间。
