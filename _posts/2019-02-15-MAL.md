---
layout: post
title: "多智能体学习" 
---

# 1、引言

在多智能体系统中，每个智能体（agent）通过与环境（environment）进行交互获得奖励值（reward）来学习改善自己的策略（policy），从而获得该环境下最优策略的过程就是多智能体强化学习。

在单智能体强化学习中，智能体所在的环境是稳定不变的，但在多智能体强化学习中，环境是复杂的、动态的，因此给学习过程带来很大的困难：

(1) 维度爆炸：强化学习一般需要存储状态值函数或状态-动作值函数。而在多智能体强化学习中，状态空间和联结动作空间（联结动作是指$$\boldsymbol{a}_t = [a_{1,t},a_{2,t},...,a_{n,t}]^T$$，其中$$a_{i,t}$$指第$i$个智能体在时刻$t$选取的动作）都是随着智能体数量呈指数级增长，因此多智能体系统维度非常大，计算复杂。

(2) 奖励函数设计困难：多智能体系统中每个智能体的目标可能不同，但是彼此之间又相互耦合影响。奖励函数设计的优劣直接影响到学习到策略的好坏。

(3) 不稳定性：在多智能体系统中，多个智能体是同时学习的。当同伴的策略改变时，每个智能体自身的最优策略也可能会改变，这将对算法的收敛性带来很大影响。

在多智能体系统中智能体之间可能会涉及到合作或竞争的关系，因此，将博弈论与强化学习相结合可以很好地处理这些问题。

# 2、博弈论基本知识

## 2.1 静态博弈

静态博弈（又称矩阵博弈或阶段博弈）是最基本的博弈模型，可以表示为$$(n,A_1,A_2,...,A_n,r_1,r_2,...,r_n)$$，其中$n$表示智能体的数量，$$A_i$$是第$i$个智能体的动作空间，$$r_i: A_1\times\cdots\times A_n\rightarrow\mathbb{R}$$表示第$i$个智能体的奖励函数，从奖励函数可以看出每个智能体获得的奖励与多智能体系统的联结动作有关，联结动作空间为$$A_1\times\cdots\times A_n$$。我们定义策略$$\boldsymbol{\pi}_i=(\pi_i(a^{(1)}),...,\pi_i(a^{(\mid A_i\mid)}))$$为智能体$i$的动作集中每个动作的概率分布（若一个策略对于智能体动作集中的所有动作的概率都大于0，则这个策略称为一个完全混合策略；若智能体的策略对一个动作的概率为1，而对其余动作的概率为0，则这个策略称为纯策略）。令$$V_i^{\boldsymbol{\pi}_1,...,\boldsymbol{\pi}_n}$$表示智能体$i$在联结策略$$[\boldsymbol{\pi}_1,...,\boldsymbol{\pi}_n]^T$$下的期望奖励，即值函数。每个智能体的目标则是最大化其获得的奖励值（值函数）。

**定义：纳什均衡**

在静态博弈中，如果联结策略$$[\boldsymbol{\pi}_1^*,...,\boldsymbol{\pi}_n^*]^T$$满足：

$$\begin{equation}V_i^{\boldsymbol{\pi}_1^*,...,\boldsymbol{\pi}_i^*,...,\boldsymbol{\pi}_n^*}\geq V_i^{\boldsymbol{\pi}_1^*,...,\boldsymbol{\pi}_i,...,\boldsymbol{\pi}_n^*}, \forall\boldsymbol{\pi}_i\in\Pi_i, i=1,...,n \label{1} \end{equation}$$

其中$$\Pi_i$$表示智能体$i$的策略空间。则称该联结策略为一个纳什均衡。如果不等式$\eqref{1}$取严格大于，则该联结策略为严格纳什均衡。

在纳什均衡处，任何智能体都不能在仅改变自身策略的情况下来获得更大的奖励。

定义$$Q_i(a_1,...,a_n)$$表示在执行联结动作$$[a_1,...,a_n]^T$$时，智能体$i$所获得的期望奖励。则纳什均衡的另一种定义方式如下：

$$\sum_{a_1,...,a_n\in A_1\times\cdots\times A_n} Q_i(a_1,...,a_n)\pi_1^*(a_1)\cdots\pi_i^*(a_i)\cdots\pi_n^*(a_n) \\ \geq \sum_{a_1,...,a_n\in A_1\times\cdots\times A_n} Q_i(a_1,...,a_n)\pi_1^*(a_1)\cdots\pi_i(a_i)\cdots\pi_n^*(a_n), \forall\boldsymbol{\pi}_i\in\Pi_i, i=1,...,n$$

### 2.1.1 双智能体静态博弈中的纳什均衡

在双智能体静态博弈中，我们可以设计一个矩阵，矩阵元素的索引坐标表示一个联结动作，第$i$个智能体的奖励矩阵$$\boldsymbol{R}_i$$的元素$$r_{xy}$$就表示第一个智能体采用动作$x$，第二个智能体采用动作$y$时第$i$个智能体获得的奖励。通常我们将第一个智能体定义为“行智能体”，第二个智能体定义为“列智能体”，行号表示第一个智能体选取的动作，列号表示第二个智能体选取的动作。对于只有2个动作的智能体，其奖励矩阵可以写为：

$$\boldsymbol{R}_1 = \begin{bmatrix}r_{11} & r_{12} \\ r_{21} & r_{22} \end{bmatrix}, \boldsymbol{R}_2 = \begin{bmatrix}c_{11} & c_{12} \\ c_{21} & c_{22} \end{bmatrix}$$


**定义：零和博弈**

如果两个智能体是完全对抗关系，即$$\boldsymbol{R}_1 = -\boldsymbol{R}_1$$，则称该博弈为零和博弈。在零和博弈中只有一个纳什均衡点，即使可能有很多纳什均衡策略，但每个智能体的奖励期望值是相同的。

一般和博弈是指任何类型的静态博弈，包括完全对抗博弈（即零和博弈）、完全合作博弈以及二者的混合博弈。在一般和博弈中可能存在多个纳什均衡点。

对于双智能体静态博弈，值函数可以表示为：

$$V_i^{\boldsymbol{\pi}_1,\boldsymbol{\pi}_2} = \boldsymbol{\pi}_1\boldsymbol{R}_i\boldsymbol{\pi}_2^T$$

纳什均衡策略$$[\boldsymbol{\pi}_1^*, \boldsymbol{\pi}_2^*]$$可以表示为：

$$V_i^{\boldsymbol{\pi}_i^*,\boldsymbol{\pi}_{-i}^*} \geq V_i^{\boldsymbol{\pi}_i,\boldsymbol{\pi}_{-i}^*}, \forall \boldsymbol{\pi}_i \in \Pi_i$$

其中$$-i$$表示除$i$以外的另一个智能体。


### 2.1.2 线性规划求解双智能体零和博弈

求解双智能体零和博弈的公式如下：

$$\max_{\boldsymbol{\pi}_i \in \Pi_i}\min_{a_{-i}\in A_{-i}}\sum_{a_i\in A_i}Q_i(a_i,a_{-i})\pi_i(a_i)$$

上式的意义为：每个智能体最大化在与对手博弈中最差情况下的期望奖励值。

奖励矩阵可写为：

$$\boldsymbol{R}_1 = \begin{bmatrix}r_{11} & r_{12} \\ r_{21} & r_{22} \end{bmatrix}, \boldsymbol{R}_2 = -\boldsymbol{R}_1$$

定义$$p_j,q_j (j=1,2)$$ 分别表示第一个和第二个智能体选择动作$j$的概率。则对于第一个智能体的纳什均衡策略，可以列写如下线性规划：

$$\begin{align}&\max_{p_1,p_2} V_1\\
	s.t.\ & r_{11}p_1 + r_{21}p_2\geq V_1 \\
	& r_{12}p_1 + r_{22}p_2 \geq V_1 \\
	& p_1 + p_2 = 1 \\
	& p_j\geq 0, j=1,2\end{align}$$

同理，可列出第二个智能体的纳什均衡策略所对应的线性规划：

$$\begin{align}&\max_{q_1,q_2} V_2\\
	s.t.\ & -r_{11}q_1 - r_{12}1_2\geq V_2 \\
	& -r_{21}q_1 - r_{22}q_2 \geq V_2 \\
	& q_1 + q_2 = 1 \\
	& q_j\geq 0, j=1,2\end{align}$$

求解上面两个问题即得到最终的纳什均衡策略。

## 2.2 随机博弈

静态博弈可以看作是在固定状态下的阶段博弈，因此在模型中实际上是省略掉了状态和阶段信息。一个随机博弈模型可以写为$$(n,S,A_1,A_2,...,A_n,p, r_1,r_2,...,r_n, \gamma)$$，其中$n$表示智能体的数量，$S$表示状态空间，$$A_i$$表示第$i$个智能体的动作空间，$$p: S\times A_1\times\cdots\times A_n\times S\rightarrow [0,1]$$表示状态转移概率，$$r_i: S\times A_1\times\cdots\times A_n\times S\rightarrow\mathbb{R}$$表示第$i$个智能体在当前状态与联结动作下得到下一状态后获得的奖励值，$\gamma$表示累积奖励折扣系数。随机博弈具有马尔可夫性，即下一阶段的状态只与当前阶段的状态与联结动作有关。因此随机博弈有时也称马尔可夫博弈。


## 2.3 重复博弈

重复博弈可以看作是随机博弈的特例，即所有智能体重复访问同一个状态的多阶段博弈。


## 2.4 动态博弈

动态博弈又称组合博弈，是指参与的智能体行动是有先后顺序的博弈，而且行动在后者可以观察到行动在先者的选择，并据此做出相应的选择，先行动者的选择会影响后行动者的选择空间。最经典的动态博弈就是下棋，通常通过博弈树来求解。

## 2.5 微分博弈

微分博弈主要用于处理时间驱动的动态系统中的博弈。

我们首先回顾一下最优控制的基础知识。被控对象的状态方程为：

$$\dot{x}(t) = f(x(t),u(t),t), x(t_0) = x_0$$

容许控制为：

$$u \in U$$

目标集表示为：

$$x(t_f)\in S$$

最小化性能指标：

$$J(u) = \int_{t_0}^{t_f} g(x(t),u(t),t)dt + h(x(t_f),t_f)$$

对于双智能体微分博弈，状态方程可表示为：

$$\dot{x}(t) = f(x(t),u_1(t),u_2(t),t), x(t_0) = x_0$$

容许控制为：

$$u_1 \in U_1, u_2\in U_2$$

二者均最小化各自的性能指标：

$$J_1(u_1,u_2) = \int_{t_0}^{t_f} g_1(x(t),u_1(t),u_2(t),t)dt + h_1(x(t_f),t_f)$$

$$J_2(u_1,u_2) = \int_{t_0}^{t_f} g_2(x(t),u_1(t),u_2(t),t)dt + h_2(x(t_f),t_f)$$



# 3、多智能体强化学习

多智能体强化学习就是在随机博弈过程中进行学习，找到对于每个状态、每个智能体的纳什均衡策略，然后将这些策略联合起来。

定义$$\pi_i : S\times A_i \rightarrow [0,1]$$为智能体$i$的策略，表示在每个状态下选择每个动作的概率。多智能体强化学习的最优策略（随机博弈的纳什均衡策略）可以写为$$[\pi_1^*,...,\pi_n^*]^T$$，且$$\forall s\in S, i=1,2,...,n$$ 满足：

$$V_i^{\pi_1^*,...,\pi_i^*,...,\pi_n^*}(s) \geq V_i^{\pi_1^*,...,\pi_i,...,\pi_n^*}(s), \forall \pi_i \in \Pi_i$$

其中，$$V_i^{\pi_1^*,...,\pi_i^*,...,\pi_n^*}(s)$$为$\gamma$折扣累积状态值函数，用$$V_i^*(s)$$来简记。我们用$$Q_i^*(s,a_1,...,a_n)$$来表示$\gamma$折扣累积状态-动作值函数，根据强化学习的Bellman公式，可得：

$$V_i^*(s) = \sum_{a_1,...,a_n\in A_1\times\cdots\times A_n} Q_i^*(s,a_1,...,a_n)\pi_1^*(s,a_1)\cdots\pi_n^*(s,a_n)$$

$$Q_i^*(s,a_1,...,a_n) =\sum_{s'\in S}p(s,a_1,...,a_n,s')[r_i(s,a_1,...,a_n,s') + \gamma V_i^*(s')]$$

多智能体强化学习的纳什均衡的另一种定义方式如下：

$$\sum_{a_1,...,a_n\in A_1\times\cdots\times A_n} Q_i^*(s,a_1,...,a_n)\pi_1^*(s,a_1)\cdots\pi_i^*(s,a_i)\cdots\pi_n^*(s,a_n) \\ \geq \sum_{a_1,...,a_n\in A_1\times\cdots\times A_n} Q_i^*(s,a_1,...,a_n)\pi_1^*(s,a_1)\cdots\pi_i(s,a_i)\cdots\pi_n^*(s,a_n), \forall\boldsymbol{\pi}_i\in\Pi_i, i=1,...,n$$

可以根据每个智能体的奖励函数对随机博弈进行分类。若智能体的奖励函数相同，则称为完全合作博弈或团队博弈。若智能体的奖励函数之和为零，则称为完全竞争博弈或零和博弈。为了求解随机博弈，需要求解每个状态$s$的阶段博弈的纳什均衡策略，该阶段博弈的奖励矩阵为$$Q_i^*(s,\cdot)$$。


