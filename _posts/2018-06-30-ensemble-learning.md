---
layout: post
title: "集成学习" 
---

# 1、个体与集成

集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统（multi-classifier system）、基于委员会的学习（committee-based learning）等。

集成学习的一般结构为：

1）先产生一组“个体学习器”（individual learner）。个体学习器通常由一种或多种现有的学习算法从训练数据中产生。如果个体学习器都是从某一种学习算法从训练数据中产生，则称这样的集成学习是同质的（homogenrous）。此时的个体学习器也称作基学习器（base learner），相应的学习算法称作基学习算法。如果个体学习器是从某几种学习算法从训练数据中产生的，则称这样的集成学习是异质的（heterogenous）。此时就不再有基学习算法，相应地，个体学习器也不称为基学习器，常称为“组件学习器”（component learner）。

2）再用某种策略将个体学习器们结合起来。集成学习通过将多个学习器组合起来，通常可以获得比单一学习器显著优越的泛化性能。

根据个体学习器的生成方式，目前的集成学习方法大致可以分作两类：一类是个体学习器之间存在强依赖关系，必须串行生成，每一轮迭代产生一个个体学习器，其中以Boosting为代表；另一类是个体学习器不存在强依赖关系，可并行生成，其中以Bagging为代表。

# 2、Boosting

Boosting是一族可将弱学习器提升为强学习器的算法，工作机制为：先从初始训练集中训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续得到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值$T$，最终将这$T$个基学习器进行加权结合。

# 3、Bagging

Bagging是并行式集成学习的典型代表，它直接基于自助采样法（bootstrap sampling）进行样本采样：先随机取出一个样本放入采样集中，再把该样本放回原始数据集，如此经过$N$次随机采样操作，得到包含$N$个样本的采样集。初始训练集中有的样本在采样集中多次出现，有的则从未出现。一个样本始终不在采样集中出现的概率是$$(1-\frac{1}{N})^N$$。根据$$\lim_{N\rightarrow\infty}(1-\frac{1}{N})^N = \frac{1}{e} \simeq 0.368$$，当样本集足够大时，初始训练集中约有$64.2\%$的样本出现在采样集中。

Bagging的基本流程为：经过$T$轮自助采样，可以得到$T$个包含$N$个训练样本的采样集，然后基于每个采样集分别训练出一个基学习器，再将这$T$个基学习器进行组合（一般对分类任务采用简单投票法，对回归任务使用简单平均法），得到集成模型。




