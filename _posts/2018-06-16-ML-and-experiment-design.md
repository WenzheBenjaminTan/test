---
layout: post
title: "机器学习及其实验设计" 
---

# 1、无监督学习与监督学习

无监督学习算法用于学习含有很多特征（feature）的数据集（dataset），然后得到这个数据集上有用的结构性质。在机器学习中，我们通常要学习用于生成数据集的整个概率分布，显示地，比如密度估计（density estimation），或是隐式地，比如生成（generation，产生一些和训练数据相似的新样本）或去噪（denoising，根据损坏的样本预测原本的样本）。还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。

监督学习算法也用于学习含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或目标（target）。

大致来说，无监督学习涉及观察随机向量$\boldsymbol{x}$的若干样本，试图显式或隐式地学习出概率分布$p(\boldsymbol{x})$，或是跟该分布相关的一些性质；而监督学习则是观察随机向量$\boldsymbol{x}$及其目标值或向量$y$，然后从$\boldsymbol{x}$预测$y$，通常是估计 $$p(y \mid \boldsymbol{x})$$。

无监督学习和监督学习并不是严格定义的术语，它们之间的界限通常是模糊的。很多机器学习技术可以同时用于这两个任务。

例如，概率的链式法则表明对于随机变量$$\boldsymbol{x} \in \mathbb{R}^d$$，联合分布可以分解为：

$$p(\boldsymbol{x}) = \prod_{i=1}^d p(x_i | x_1,x_2,...,x_{i-1}) $$

该分解意味着我们可以将其拆分为$d$个监督学习问题，来解决表面上的无监督学习$p(\boldsymbol{x})$。

同样地，我们求解监督学习问题的$$p(y\mid \boldsymbol{x})$$时，也可以使用传统的无监督学习方法来学习联合分布$p(\boldsymbol{x},y)$，然后推断：

$$p(y | \boldsymbol{x}) = \frac{p(\boldsymbol{x},y)}{\sum_{y'}p(\boldsymbol{x},y')}$$

对于监督学习，当给定有限的训练样本集，如是通过直接建模$$p(y\mid \boldsymbol{x})$$来预测$y$，这样得到的是“判别式模型”（discriminative model）；当是先对联合分布$p(\boldsymbol{x},y)$建模，然后再由此获得$$p(y\mid \boldsymbol{x})$$时，这样得到的是“生成式模型”（generative models）。

尽管无监督学习和监督学习并非完全不相关，但它们确实有利于粗略区分我们研究机器学习时遇到的问题。传统上，人们将回归、分类或结构化输出问题称为监督学习，将支持其他任务的密度估计相关问题称为无监督学习。

学习范式的其他变种也是可能的。例如半监督学习中，一些样本有监督目标，但其他样本没有。

大部分机器学习算法都简单地被训练于一个固定的数据集上。数据集可以用很多不同方式来表示，但是在所有情况下，数据集都是样本的集合，而样本是特征的集合。有些机器学习算法并不是用于一个固定的数据集上。例如，强化学习算法会和环境交互，所以学习系统和它的训练过程会有反馈回路。


# 2、机器学习实验设计

