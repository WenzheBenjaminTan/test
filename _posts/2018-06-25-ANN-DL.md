---
layout: post
title: "人工神经网络和深度学习" 
---
# 1、神经元

构成神经网络的基本单元称为神经元，主要包括三个部分：输入权重参数$\mathbf{w}$，加法门$\sum$以及激活函数$\sigma$。其中输入权重$\mathbf{w}$作用于输入$\mathbf{x}$（也可能是上一层网络的输出），然后经过加法门$\sum$将其加起来得到一个和，最后经过激活函数$\sigma$得到一个激活值。激活函数可以看作是一个阈值处理装置，只有输入超过一定的值后才会有一个输出。多个并行的神经元可以构成一个深度神经网络的网络层，而多层的神经网络便构成了一个深度神经网络。

# 2、全连接神经网络（Fully Connected Neural Networks，FCNN）

全连接神经网络是最普遍的一种网络结构。根据功能的不同，可以将网络层分为三种：一种叫作输入层，使用样本的特征向量作为输入和输出（可以认为是一个单位函数）；一种叫作输出层，按照网络的设计输出想要的结果；输入层和输出层可以看作是一种没有功能的网络层，只是作为一个接口，而处于输入层和输出层之间的功能网络层被称为隐藏层，能够对于上层的输入进行特定功能的处理。深度神经网络是指至少具备一个隐含层的神经网络，所谓的深度也就是指指网络结构中隐含层很深。狭义上的深度神经网络（Deep Neuron Networks，DNN）是指只有全连接的网络结构，而广义上的DNN还包含其他的网络结构，如卷积网络、循环网络、生成式网络等。

一般地，我们使用$w_{jk}^l$表示从第$l-1$层的第$k$个神经元到第$l$层的第$j$个神经元的连接上的权重，$$b_j^l$$和$a_j^l$分别表示第$l$层的第$j$个神经元的偏置和激活值。

有了上述表示，第$l$层的第$j$个神经元的激活值$a_j^l$就和第$l-1$层的激活值关联起来了，即：

$$a_j^l = \sigma\left(\sum_{k}w_{jk}^la_k^{l-1} + b_j^l\right)$$

为了用矩阵的形式重写这个式子，我们定义权重矩阵$\boldsymbol{w}^l$来表示第$l$层所有神经元连接的权重，该矩阵的第$j$行、第$k$列的值即为$w_{jk}^l$。同样地，定义$$\boldsymbol{b}^l$$和$\boldsymbol{a}^l$分别表示第$l$层神经元的偏置和激活值向量。于是上式可以改写为：

$$\boldsymbol{a}^l = \sigma\left(\boldsymbol{w}^l\boldsymbol{a}^{l-1} + \boldsymbol{b}^l\right)$$

去掉了那些神经元序号$k$和$j$，这个式子帮助我们更宏观地理解上一层神经元的输出是怎么影响下一层神经元的。我们将权重矩阵乘以上一层神经元的输出，再加上这一层神经元自身的偏置，再经过激活函数作用，得到的就是这一层神经元的输出。

此外，我们还可以定义第$l$层的加权输入：$$\boldsymbol{z}^l = \boldsymbol{w}^l\boldsymbol{a}^{l-1} + \boldsymbol{b}^l$$，即$$z_j^l = \sum_{k}w_{jk}^la_k^{l-1} + b_j^l$$，也就是在激活函数处理之前的部分。于是可以得到$$\boldsymbol{a}^l = \sigma(\boldsymbol{z}^l)$$。


理想中的激活函数是一个阶跃函数，即

$$\sigma(x) = sgn(x) = \begin{cases}1 & x\geq 0 \\ 0 & x < 0 \end{cases}$$

它将输入值映射为输出值“0”或“1”，显然“1”对应于神经元兴奋，“0”对应于神经元抑制。然而，阶跃函数具有不连续、不光滑等不好的性质，因此常用S型函数作为激活函数，即

$$\sigma(x) = sigmoid(x) = \frac{1}{1 + e^{-x}}$$

它把可能在较大范围内变化的输入值挤压到(0,1)输出范围内，因此有时又称为“挤压函数”（squashing function），对应的神经元叫S型神经元。

# 3、误差反向传播算法（error BackPropagation，BP）

考虑二次损失函数：

$$C(\boldsymbol{w}, \boldsymbol{b}) =\frac{1}{2N} \sum_{i=1}^N \| \mathbf{y}_i - \boldsymbol{a}^L(\mathbf{x}_i)\|^2$$

其中$N$是训练样本总数，$\mathbf{y}_i$是输入为$$\mathbf{x}_i$$时对应的真实输出，而$L$表示神经网络的层数，即$$\boldsymbol{a}^L(\mathbf{x}_i)$$表示输入为$$\mathbf{x}_i$$时神经网络的输出。

采用梯度下降的思想，误差反向传播算法实际是计算损失函数的偏导数：$$\frac{\partial C}{\partial w_{jk}^l}$$ 和 $$\frac{\partial C}{\partial b_j^l}$$，即理解改变权重和偏置将会如何影响损失函数值，从而得到更新公式：

$${w_{jk}^l} \leftarrow w_{jk}^l - \eta \frac{\partial C}{\partial w_{jk}^l}$$

$${b_j^l} \leftarrow b_j^l - \eta \frac{\partial C}{\partial b_j^l}$$

其中$\eta$为更新步长。


为了介绍如何计算这些偏导数，首先引入一个中间变量$$\delta_j^l$$，称其为第$l$层的第$j$个神经元的误差（error），然后再将其与需要计算的偏导数关联起来。$$\delta_j^l$$定义为：

$$\delta_j^l = \frac{\partial C}{\partial z_j^l}$$

它的绝对值表示该神经元的加权输入的改变对损失函数值（总体误差）的影响程度或灵敏度。

1）公式一：输出层的error

$$\delta_j^L = \frac{\partial C}{\partial a_j^L} \sigma'(z_j^L)$$

这个式子通过链式法则不难得到。写成矩阵形式为：

$$\begin{equation}\label{1} \boldsymbol{\delta}^L = \nabla_{\boldsymbol{a}^L} C \odot \sigma'(\boldsymbol{z}^L)\end{equation}$$

其中$\odot$表示Hadamard乘积。

对于二次损失函数，容易得到：

$$ \boldsymbol{\delta}^L = \frac{1}{N} \sum_{i=1}^N (\boldsymbol{a}^L(\mathbf{x}_i) - \mathbf{y}_i) \odot \sigma'(\boldsymbol{z}^L)$$

2）公式二：用当前层的error表示上一层的error

由定义，可知

$$z_k^{l+1} = \sum_{j}w_{kj}^{l+1}a_j^{l} + b_k^{l+1} = \sum_{j}w_{kj}^{l+1}\sigma(z_j^{l}) + b_k^{l+1}$$

于是有

$$\delta_j^l = \frac{\partial C}{\partial z_j^l} = \sum_k\frac{\partial C}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l} = \sum_k w_{kj}^{l+1}\delta_k^{l+1}\sigma'(z_j^l)$$

写成矩阵形式为：

$$\begin{equation}\label{2} \boldsymbol{\delta}^l = (\boldsymbol{w}^{l+1})^T\boldsymbol{\delta}^{l+1} \odot \sigma'(\boldsymbol{z}^l) \end{equation}$$

3）公式三：error等价于损失函数对偏置的变化率

$$\frac{\partial C}{\partial b_j^l} = \delta_j^l$$

这个式子通过链式法则不难得到。又可以写成：

$$\begin{equation}\label{3} \frac{\partial C}{\partial b} = \delta\end{equation}$$

4）公式四：损失函数对权重的变化率

$$ \frac{\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l $$

这个式子也很容易通过链式法则得到。又可以写成：

$$\begin{equation}\label{4} \frac{\partial C}{\partial w} = a_{\text{in}}\delta_{\text{out}}\end{equation}$$

以上四个公式给出了一种计算损失函数梯度的方法，我们显示地描述反向传播算法如下：

1）**输入训练样本的集合（大小为$m$）**。

2）**对于每个训练样本$\boldsymbol{x}$**：设置对应的输入层激活值$\boldsymbol{a}^{\boldsymbol{x},1}$，并执行以下步骤：

+ **前向传播**：对每个$l=2,3,...,L$计算相应的$$\boldsymbol{z}^{\boldsymbol{x},l} = \boldsymbol{w}^l\boldsymbol{a}^{\boldsymbol{x},l-1} + \boldsymbol{b}^l$$和$$\boldsymbol{a}^{\boldsymbol{x},l} = \sigma(\boldsymbol{z}^{\boldsymbol{x},l})$$。

+ **计算输出层误差$\boldsymbol{\delta}^{\boldsymbol{x},L}$**：计算向量$$ \boldsymbol{\delta}^{\boldsymbol{x},L} = \nabla_{\boldsymbol{a}^{\boldsymbol{x},L}} C_{\boldsymbol{x}} \odot \sigma'(\boldsymbol{z}^{\boldsymbol{x},L})$$。

+ **误差反向传播**：对每个$l=L-1,L-2,...,2$计算$$\boldsymbol{\delta}^{\boldsymbol{x},l} = (\boldsymbol{w}^{l+1})^T\boldsymbol{\delta}^{\boldsymbol{x},l+1} \odot \sigma'(\boldsymbol{z}^{\boldsymbol{x},l})$$。

3）**梯度下降**：对每个$l=L-1,L-2,...,2$根据$$\boldsymbol{w}^l \leftarrow \boldsymbol{w}^l - \frac{\eta}{m} \sum_{\boldsymbol{x}} \boldsymbol{\delta}^{\boldsymbol{x},l}(\boldsymbol{a}^{\boldsymbol{x},l-1})^T$$和$$\boldsymbol{b}^l \leftarrow \boldsymbol{b}^l - \frac{\eta}{m}\sum_{\boldsymbol{x}} \boldsymbol{\delta}^{\boldsymbol{x},l}$$得出。

# 4、让神经网络更快地学习

从公式\eqref{4}可以看出，当$a_{\text{in}} \approx 0$时，梯度$\frac{\partial C}{\partial w}$也会趋向很小，这样我们就说权重**缓慢学习**，即在梯度下降时，这个权重不会改变太多。

如果激活函数采用的S型函数，当$\sigma(z_j^L)$近似为0或为1的时候，$\sigma$函数变得非常平，因此$\sigma'(z_j^L)\approx 0$，根据公式\eqref{1}可知$\delta_j^L$也会趋向很小，最终层的权重和偏置学习也会非常缓慢。这种情形，我们常常称输出神经元已经**饱和**了。根据公式\eqref{2}可知，针对前面的层，我们也有类似的观点。

总结一下，如果输入神经元激活值很低，或者输出神经元已经饱和（过高或过低的激活值），权重学习会非常缓慢。

## 4.1 引入交叉熵损失函数

假设我们要训练一个包含若干输入变量$x_1,x_2,...$的神经元，对应的权重为$w_1,w_2,...$，偏置为$b$。神经元的输出就是$$a=\sigma(z)$$，其中$$z=\sum_jw_jx_j+b$$是输入的带权和。我们定义这个神经元的交叉熵损失函数为：

$$C = -\frac{1}{N}\sum_{\boldsymbol{x}}\left(y\log a + (1-y)\log (1-a)\right)$$

其中$N$是训练数据的总数，求和是在所有训练输入$\boldsymbol{x}$上进行的，$y$是对应的目标输出。

用S型函数作为激活函数时，交叉熵可以看作是损失函数有两个原因：第一，它是非负的，即$C > 0$；第二，当神经元的实际输出等于目标值，即$\sigma(z) = y$时，它能达到最小值。当然这两个特性也是二次损失函数具备的。但是交叉熵有一个比二次损失函数更好的特性就是它避免了学习速度下降的问题。

当激活函数为S型函数时，容易得到：

$$\sigma'(z) = \sigma(z)(1-\sigma(z))$$

所以可以得到：

$$\frac{\partial C}{\partial w_j} = \frac{1}{N}\sum_{\boldsymbol{x}}x_j(\sigma(z)-y)$$

$$\frac{\partial C}{\partial b} = \frac{1}{N}\sum_{\boldsymbol{x}}(\sigma(z)-y)$$

从上可以看出，交叉熵作为损失函数时权重学习的速度不再受$\sigma'(z)$的控制，而且其受$\sigma(z)-y$，也就是输出中的误差的控制。更大的误差，更快的学习速度，这正是我们期待的结果。

我们可以进一步将交叉熵推广到有很多个神经元多层神经网络上。设$y_1,y_2,...$是输出神经元的目标值，而$a_1^L,a_2^L,...$是实际输出值，那么我们定义交叉熵如下：

$$C = -\frac{1}{N}\sum_{\boldsymbol{x}}\sum_j\left(y_j\log a_j^L + (1-y_j)\log (1-a_j^L)\right)$$

对于交叉熵损失函数，针对一个训练样本$\boldsymbol{x}$的输出误差$\boldsymbol{\delta}^L$为

$$\boldsymbol{\delta}^L = \boldsymbol{a}^L-\boldsymbol{y}$$

输出层的权重和偏置的偏导数为：

$$\frac{\partial C}{\partial w_{jk}^L} = \frac{1}{N}\sum_{\boldsymbol{x}}a_k^{L-1}(a_j^L-y_j)$$

$$\frac{\partial C}{\partial b_{j}^L} = \frac{1}{N}\sum_{\boldsymbol{x}}(a_j^L-y_j)$$


我们应该在什么时候用交叉熵来替代二次损失函数呢？实际上，如果在神经元是S型神经元时，交叉熵一般都是更好的选择。为什么？考虑我们在初始化网络权重和偏置时通常使用某种随机方法，可能会发生这种情况：这些初始选择会对某些训练输入有明显的误差，比如说，目标输出是1，而实际值是0，或者完全相反。如果这时我们使用的二次损失函数，就会导致学习速度非常缓慢。

## 4.2 在输出层使用线性神经元时使用二次损失函数

假设我们有一个多层多神经元网络，最终输出层的神经元都是**线性神经元**，即输出不再是S型函数作用的结果，而是$$a_j^L = z_j^L$$。如果我们使用二次损失函数，那么针对一个训练样本$\boldsymbol{x}$的输出误差$\boldsymbol{\delta}^L$也变为了

$$\boldsymbol{\delta}^L = \boldsymbol{a}^L-\boldsymbol{y}$$

输出层的权重和偏置的偏导数也变为：

$$\frac{\partial C}{\partial w_{jk}^L} = \frac{1}{N}\sum_{\boldsymbol{x}}a_k^{L-1}(a_j^L-y_j)$$

$$\frac{\partial C}{\partial b_{j}^L} = \frac{1}{N}\sum_{\boldsymbol{x}}(a_j^L-y_j)$$

这表明如果输出神经元是线性的，那么二次损失函数不再会导致学习速度下降的问题。在此情形下，二次损失函数就是一个合适的选择。

## 4.3 Softmax输出层

Softmax的想法其实就是为神经网络定义一种新式的输出层。开始时是和S型层一样的，首先计算带权输入：

$$z_j^L = \sum_kw_{jk}^{L}a_k^{L-1} + b_j^L$$

不过这里我们不会使用S型函数来获得输出，而是会在这一层上应用一种叫做Softmax的函数在$z_j^L$上，根据这个函数，第$j$个神经元的激活值为：

$$a_j^L = \frac{e^{z_j^L}}{\sum_k e^{z_k^L}}$$

其中，分母中的求和是在所有的输出神经元上进行的。

根据定义，输出层的激活值均为正，且所有和正好为1。因此，Softmax层的输出可以被看作是一个概率分布，激活值$a_j^L$代表网络对于正确输出为$j$的概率估计。

对于Softmax输出层，我们还要定义**对数似然（log-likelihood）**损失函数：

$$C = -\frac{1}{N}\sum_{\boldsymbol{x}}\log a_y^L$$

其中$N$是训练数据的总数，求和是在所有训练输入$\boldsymbol{x}$上进行的，$y$是对应的目标输出。

针对一个训练样本$\boldsymbol{x}$的输出误差$\boldsymbol{\delta}^L$为：

$$\boldsymbol{\delta}^L = \boldsymbol{a}^L-\boldsymbol{y}$$

进而可得到：

$$\frac{\partial C}{\partial w_{jk}^L} = \frac{1}{N}\sum_{\boldsymbol{x}}a_k^{L-1}(a_j^L-y_j)$$

$$\frac{\partial C}{\partial b_{j}^L} = \frac{1}{N}\sum_{\boldsymbol{x}}(a_j^L-y_j)$$

这些方程和我们前面得到的是一样的，正如前面所分析，这些表达式确保我们不会遇到学习缓慢的问题。事实上，一个具有对数似然损失函数的Softmax输出层，与一个具有交叉熵损失函数的S型输出层非常相似。从通用的视角来看，交叉熵加S型输出层的组合更通用一些，而对数似然加Softmax的组合更加适用于那些需要将输出激活值解释为概率的场景。

## 4.4 更好地权重初始化

前面讨论的是如何解决输出神经元在错误的值上饱和导致学习下降的问题，但并不能解决隐藏神经元饱和的问题。如果按照独立高斯随机变量来初始化权重和偏置，其被标准化为均值为0，标准差为1。假设我们使用一个有大量输入神经元的网络，比如说1000个，我们已经使用标准化的高斯分布初始化了连接第一个隐藏层的权重。为了简化，假设我们使用的训练输入在一半的输入神经元值为1，另一半为0。让我们考虑第一层隐藏神经元的输入带权和$$z = \sum_j w_j x_j + b$$。因为有500个项为0，所以$z$是遍历501个标准化高斯随机变量的和（500个权重和1个额外偏置项）。因此，$z$本身是一个均值为0，标准差为$\sqrt{501}$的高斯分布，因此$\mid z\mid$可能会变得非常大，隐藏神经元就会饱和。当出现这样的情况时，在权重中进行微小的调整仅仅会给隐藏神经元的激活值带来极其微弱的变化，而这种微弱的变化也会影响网络中剩下的神经元。结果就是，这些权重在我们进行梯度下降算法时学习得非常缓慢。

因此，为了更好地初始化权重，我们可以如下做：假设有一个有$n_{\text{in}}$个输入权重的神经元，使用均值为0，标准差为$\frac{1}{\sqrt{n_{\text{in}}}}$的高斯随机分布初始化这些权重。也就是说，我们会向下挤压高斯分布，让我们的神经元更不可能饱和。

对于偏置可以继续用均值为0，标准差为1的高斯分布进行初始化。实际上，考虑到已经避免了饱和的问题，如何初始化偏置影响不大。有时候可以直接将所有偏置初始化为0，依赖梯度下降来学习合适的偏置。

# 5、过度拟合及其预防

## 5.1 过度拟合

拥有大量自由参数的模型能够描述一些特别神奇的现象。即使这样的模型能够很好地拟合已有数据，但并不表示是一个好模型。因为这可能只是因为模型中足够的自由度使得它可以描述几乎所有给定大小的数据集，而不需要真正洞察数据背后的规律和本质。所以发生这种情形时，模型对已有数据会表现得很好，但是对新的数据很难泛化。我们称这种现象为**过度拟合（overfitting）**。

过度拟合是人工神经网络的一个主要问题。为了高效地训练，我们需要一种检测过度拟合是否发生的技术，这样我们就不会过度训练。并且，我们也想找到一些技术来降低过度拟合的影响。

检测过度拟合最直接的方法就是，跟踪测试数据集合上的准确率随训练变化的情况。如果我们看到测试数据上的准确率不再提升，那么我们就停止训练。严格地说，这其实并非是过度拟合的一个必要现象，因为测试集和训练集上的准确率可能会同时停止提升。当然，采用这种方法是可以阻止过度拟合的。

一般来说，最好的降低过度拟合的方式之一就是增加训练样本的量。有了足够的训练数据，就算是一个规模非常大的网络也不容易过度拟合。不幸的是，训练数据其实是很难或者很昂贵的资源，所以这不是一种太切实际的选择。

## 5.2 正则化（regularization）

除了增加训练样本或者降低网络规模，我们还有其他技术能够缓解过度拟合，正则化就是典型的代表。

最常用的正则化手段叫作权重衰减（weight decay）或$$L_2$$正则化。$$L_2$$正则化的想法是增加一个额外的项到损失函数上，这个项叫正则化项。下面是正则化的二次损失函数：

$$C = \frac{1}{2N}\sum_{\boldsymbol{x}}\|\boldsymbol{y} - \boldsymbol{a}^L\|^2 + \frac{\lambda}{2N}\sum_w w^2$$

其中第一项就是常规的损失函数表达式，第二项加进去的是所有权重的平方和，然后使用一个因子$\frac{\lambda}{2N}$进行量化调整，其中$\lambda > 0$称为正则化参数，而$N$就是训练集合的大小。当然，也可以对其他损失函数进行正则化，比如交叉熵：

$$C = -\frac{1}{N}\sum_{\boldsymbol{x}}\left(y\log a + (1-y)\log (1-a)\right) + \frac{\lambda}{2N}\sum_w w^2$$

两者可以统一写成这样：

$$C = C_0 + \frac{\lambda}{2N}\sum_w w^2$$

其中$C_0$是原始的损失函数。

直观地看，正则化的效果是让网络倾向于学习小一点的权重，其他东西都是一样的。大的权重只有能够给出损失函数第一项足够的减少时才被允许。换言之，正则化可以当作一种寻找小的权重和最小化原始损失函数之间的折中。这两部分之间的相对重要性就由$\lambda$的值来控制了：$\lambda$小，就倾向于最小化原始损失函数，反之，倾向于小的权重。

容易得到：

$$\frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w} + \frac{\lambda}{N}w$$

$$\frac{\partial C}{\partial b} = \frac{\partial C_0}{\partial b}$$

所以偏置的梯度下降学习规则不会发生变化，而权重的学习规则变成：

$$w \leftarrow w - \eta \frac{\partial C_0}{\partial w} - \frac{\eta\lambda}{N}w = (1-\frac{\eta\lambda}{N})w - \eta \frac{\partial C_0}{\partial w}$$

它的变化仅仅是通过一个因子$$1-\frac{\eta\lambda}{N}$$重新调整了权重$w$，这种调整正是权重衰减的名称来源。

为什么正则化可以帮助减轻过度拟合？假设神经网络有很小的权重，这最可能出现在正则化的网络中。更小的权重意味着网络的行为不会因为我们随便改变了一个输入而改变太大。这会让正则化网络学习局部噪声的影响更加困难。因此，可以将正则化看作是一种让单个证据不会影响网络输出太多的方式。对比来看，大权重的网络可能会因为输入的微小改变而产生较大的行为改变。所以一个无正则化的网络可以使用大的权重来学习包括训练数据中噪声的大量信息的复杂模型。简言之，正则化网络因为受限于根据训练数据中常见的模式来构造相对简单的模型，而能够抵抗训练数据中噪声的特性影响，因此可以更好地进行泛化。

为什么不对偏置进行正则化？因为有一个大的偏置并不会像大的权重那样让神经元对输入太过敏感，所以我们不需要对大的偏置所带来的学习训练数据的噪声太过担心。同时，允许大的偏置能够让网络更加灵活。因为大的偏置让神经元更加容易饱和，这有时候是我们所要达到的效果。所以，我们通常不会对偏置进行正则化。

## 5.3 Dropout

与正则化技术不同，Dropout并不依赖对损失函数的修改，而是在学习过程中调整网络本身。

假设我们尝试训练一个三层网络：

特别地，假设我们有一个训练数据$\boldsymbol{x}$和对应的目标输出$y$。通常我们会通过在网络中前向传播$\boldsymbol{x}$，然后进行误差反向传播。使用Dropout，这个过程就改了。我们会从随机且临时地dropout网络中的一半隐藏神经元开始，同时让输入层和输出层的神经元保持不变。前向传播输入$\boldsymbol{x}$，通过修改后的网络，然后反向传播误差。在小批量样本上进行这些步骤后，我们得到调整后的权重和偏置。然后重复这个过程，首先重置dropout的神经元，然后选择一个新的随机的隐藏神经元子集进行dropout，对小批量数据进行梯度估计，然后更新权重和偏置。

通过不断地重复，我们的网络会学到一个新的权重和偏置集合。当然，这些权重和偏置都是在一半的隐藏神经元被dropout的情况下学到的。我们实际运行时，两倍的隐藏神经元将会被激活。因此，为了补偿，我们将所有隐藏神经元的输出权重减半，于是得到了学习后的最终网络。

Dropout为什么可以减轻过度拟合？启发式地看，当我们dropout不同的神经元时，相当于我们在训练不同的神经网络，所以dropout方法就相当于大量不同网络效果的平均。不同网络会以不同方式过度拟合，所以经过dropout得到的最终网络的效果会减轻过度拟合。


# 6、深度学习的障碍

## 6.1 深度神经网络中的梯度不稳定性

我们首先来看一个极简单的深度神经网络：每一层都只有一个单一的神经元，共有三个隐藏层，权重依次为$w_1,w_2,w_3,w_4$，偏置依次为$b_1,b_2,b_3,b_4$。回顾一下，从第$j$个神经元的输出$a_j = \sigma(z_j)$，其中$\sigma$是常用的S型激活函数，而$z_j = w_ja_{j-1} + b_j$是神经元的带权输入。损失函数$C$是网络输出$a_4$的函数：如果实际输出越接近目标输出，那么损失会变低；相反则会变高。

现在我们研究一下第一个隐藏神经元的梯度$\frac{\partial C}{\partial b_1}$：

$$\frac{\partial C}{\partial b_1} = \sigma'(z_1)\times w_2\times \sigma'(z_2)\times w_3\times \sigma'(z_3)\times w_4\times \sigma'(z_4)\times \frac{\partial C}{\partial a_4}$$

表达式结构如下：对每个神经元有一个$\sigma'(z_j)$项；对每个权重有一个$w_j$项；还有一个$\frac{\partial C}{\partial a_4}$项，表示最后的损失函数相对网络输出的导数。

根据Sigmoid函数的导数图像可知，在$\sigma'(0)=0.25$时达到最高。如果我们用标准方法来初始化网络中的权重，那么会使用一个均值为0，标准差为1的高斯分布，因此所有权重通常会满足$\mid w_j \mid < 1$。于是可得$\mid w_j\sigma'(z_j)\mid < 0.25$，当我们对所有这些项进行项的乘积时，最终结果肯定会指数级下降，因此出现在BP的时候**梯度消失**现象。特别地，如果权重$w_j$在训练中进行了增长，使得$\mid w_j\sigma'(z_j)\mid > 1$，那这时又会发生BP的时候**梯度激增**现象。

由于前面层的梯度是来自于后面层上各项$$w_j\sigma'(z_j)$$的乘积，当存在过多的层次时，就出现了内在本质上的梯度不稳定场景。所以如果使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。唯一让所有层都接近相同学习速度的方式是所有这些项的乘积都能得到一种平衡。

对于复杂的深层网络，我们可以通过反向传播算法得到在一个共$L$层网络中第$l$层的梯度：

$$\boldsymbol{\delta}^l = \Sigma'(\boldsymbol{z}^l)(\boldsymbol{w}^{l+1})^T\Sigma'(\boldsymbol{z}^{l+1})\cdots (\boldsymbol{w}^L)^T\Sigma'(\boldsymbol{z}^L)\nabla_{\boldsymbol{a}^L}C$$

其中，$$\Sigma'(\boldsymbol{z}^j)$$是一个对角矩阵，每个元素是对第$j$层的带权输入计算Sgimoid导数$$\sigma'(z)$$，而$$\boldsymbol{w}^j$$是对第$j$层的权值矩阵，$$\nabla_{\boldsymbol{a}^L}C$$是对每个输出激活值的偏导数向量。

这时更加复杂的表达式，但本质上是相似的。主要是包含了形如$$(\boldsymbol{w}^j)^T\Sigma'(\boldsymbol{z}^j)$$的项，而且$$\Sigma'(\boldsymbol{z}^j)$$在对角线上的值很小，不会超过0.25。由于权值矩阵$$\boldsymbol{w}^j$$初始时不是太大，每个额外的项$$(\boldsymbol{w}^j)^T\Sigma'(\boldsymbol{z}^j)$$会让梯度向量更小，导致梯度消失。更加一般地看，在乘积中大量的项会导致不稳定的梯度。实践中，一般会发现在Sigmoid网络中前面的层的梯度呈指数级的消失，所以这些层上的学习速度就会变得非常慢。这种减速不是偶然现象：也是我们采用的训练方式决定的。

## 6.2 其他障碍

不稳定的梯度仅仅是深度学习的众多障碍之一，尽管这一点是相当根本的。实际上，激活函数的选择，权重的初始化，甚至是学习算法的实现方式也扮演了重要角色。当然，网络结构和其他超参数本身也是很重要的。



# 7 卷积神经网络（Convolutional Neural Networks，CNN）

使用全连接层的网络来分类图像是很奇怪的，因为其网络架构不考虑图像的空间结构，例如，它在完全相同的基础上去对待距离很远和彼此接近的输入像素。因此，图像空间结构的概率必须从训练数据中来推断。而卷积神经网络则可以利用空间结构，是一种特别适用于分类图像的深度神经网络。

## 7.1 卷积网络中的三个基本概念

卷积神经网络主要采用三种基本概念：局部感受野（local receptive fields），共享权重（shared weights），和池化（pooling）。

1）局部感受野

在卷积神经网络中，我们把输入像素连接到一个隐藏神经元层，但是不会把每个输入像素连接到每个隐藏神经元。相反，我们只是把输入图像进行小的、局部区域的连接。

这个输入图像的区域被称作隐藏神经元的局部感受野，它是输入像素上的一个小窗口。每个连接学习一个权重，而隐藏神经元同时也学习一个总的偏置。可以把这个特定的隐藏神经元看作是在学习分析它的局部感受野。

我们然后在整个输入图像上交叉移动局部感受野。对于每个局部感受野，在第一个隐藏层中对应一个不同的隐藏神经元。一般来说，局部感受野每次移动一个像素。但需要指出的是，有的时候会使用不同的跨距。

例如，对于输入像素为$28\times 28$的图像，我们可以使用$5\times 5$的局部感受野，于是可以构建出具有$24\times 24$个神经元的第一个隐藏层。

2）共享权重和偏置

我们已经说过每个隐藏神经元具有一个偏置和连接到它的局部感受野的$5\times 5$权重，没有提及的是我们打算对$24\times 24$隐藏神经元中的每一个都使用相同的权重和偏置。

也即，对第$j, k$个隐藏神经元，输出为：

$$\begin{equation}\sigma\left(\sum_{l=0}^4\sum_{m=0}^4 w_{l,m}a_{j+l,k+m} + b \right) \label{5}\end{equation}$$

其中，$\sigma$是神经元的激活函数，$w_{l,m}$表示共享权重的$5\times 5$数组，$a_{x,y}表示位置为$x,y$的输入激活值，$$b$是共享偏置的值。之所以称其为“卷积神经网络”，是因为方程\eqref{5} 中的操作符被称为卷积（convolution）。因此，又可以把这个方程写成为：

$$\boldsymbol{a}^1 = \sigma(\boldsymbol{w}*\boldsymbol{a}^0 + b)$$

其中$\boldsymbol{a}^1$表示输出激活值矩阵，$\boldsymbol{a}^0$是输入激活值矩阵，而$*$表示卷积操作。

这意味着第一个隐藏层的所有神经元检测完全相同的“特征”，只是在输入图像的不同位置。在图像中应用相同的特征检测器是非常有用的，用稍微抽象的术语，卷积网络能很好地适应图像的平移不变性。例如，稍稍移动一幅猫的图像，它仍然是一幅猫的图像。

因为这个原因，我们有时候把从输入层到第一个隐藏层的映射称为**特征映射**，共享权重和偏置经常被称为一个**卷积核**或**滤波器**。

为了完成图像识别我们往往需要超过一个的特征映射，因此一个完整的卷积层由几个并行的不同特征映射组成。

共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数。对于每个特征映射我们需要$5\times 5=25$个共享权重，加上一个共享偏置，所以每个特征映射只需要26个参数。如果我们有20个特征映射，那么总共有$26\times 20 = 520$个参数来定义卷积层。作为对比，假设我们有一个全连接的第一层，具有$28\times 28 = 784$个输入神经元，和一个相对适中的30个隐藏神经元，则总共有$784\times 30$个权重，加上额外的30个偏置，共有23550个参数。换句话说，这个全连接层有多达40倍于卷积层的参数。


3）池化

除了刚刚描述的卷积层，卷积神经网络还包括池化层（pooling layers）。池化层通常紧接着卷积层之后使用，它的主要作用是简化卷积层输出的信息。

具体来说，一个池化层首先取得卷积层输出的所有特征映射信息并且对他们进行一个凝缩的特征映射。一个常见的池化方法被称为最大值池化（max-pooling）。例如，池化层的每个单元可能概括了前一层的一个$2\times 2$区域，在最大值池化操作中，一个池化单元简单地输出其$2\times 2$输入区域的最大激活值。如果上一层卷积层有$24\times 24$个神经元输出，池化后我们得到$12\times 12$个神经元。

正如前面提到，卷积层通常包含超过一个特征映射，我们需要将最大值池化分别作用于每一个特征映射。

我们可以把最大值池化看作是一种向图像中某一区域询问是否有某个给定特征的方式，且丢弃其确定的位置信息。实际上，一旦一个个特征被发现了，它的确切位置并不如它相对于其他特征的大概位置信息重要。通过池化，能够减少特征信息的数量，从而有助于减少后面层需要参数的数量。

## 7.2 卷积网络中的反向传播

### 7.2.1 CNN反向传播的不同之处

首先需要注意的是，一般神经网络中每一层的输入输出$\boldsymbol{a}$、$\boldsymbol{z}$都只是一个向量，而在CNN中，$\boldsymbol{a}$、$\boldsymbol{z}$是一个三维张量，即由若干个子矩阵组成。

其次：

1）池化层没有激活函数。这个问题比较好解决，我们可以令池化层的激活函数为$\sigma(\boldsymbol{z}) = \boldsymbol{z}$，因此导数为1。

2）池化层在前向传播的时候对输入进行了压缩。因此，我们在反向推导上一层误差时需要做upsample处理。

3）卷积层是通过张量卷积，或者说若干个矩阵分别进行卷积运算而得到当前层的输出，这和一般神经网络直接进行矩阵乘法得到当前层的输出不同。因此，在卷积层反向传播的时候，上一层误差的递推方式也不同。

4）对于卷积层，由于$\boldsymbol{w}$使用的运算是卷积，那么由该层误差推导出该层的所有卷积核的权值$\boldsymbol{w}$和偏置$\boldsymbol{b}$的方式也不同。

由于卷积层可以有多个卷积核，各个卷积核的处理方式是完全相同且独立的，为了简化计算公式的复杂度，我们下面提到的卷积核都是卷积层中若干卷积核中的一个。

### 7.2.2 已知池化层的误差，反向推导上一隐藏层的误差

在前向传播时，池化层会用MAX或者MEAN操作对输入进行池化，且池化的区域大小已知。现在我们反过来，要从缩小后区域的误差，还原上一层较大区域的误差。

首先要进行unsample。

假设池化层$l$的误差矩阵$$\boldsymbol{\delta}^l$$为：

$$\boldsymbol{\delta}^L = \left( \begin{matrix}2 & 8 \\ 4 & 6 \end{matrix}\right)$$

如果池化区域表示为$a\times a$大小，那么我们把上述矩阵上下左右各扩展$a-1$行和列进行还原，假设$a=2$，则得到：

$$\left( \begin{matrix}0 & 0 & 0 & 0 \\ 0 & 2 & 8 & 0 \\ 0 & 4 & 6 & 0 \\ 0 & 0 & 0 & 0 \end{matrix}\right)$$

如果前向传播是采用的MAX操作，则将每个误差放到前向传播时记录的最大值位置，假设分别为左上、右下、右上、左下，unsample后的矩阵为：

$$\left( \begin{matrix}2 & 0 & 0 & 0 \\ 0 & 0 & 0 & 8 \\ 0 & 4 & 0 & 0 \\ 0 & 0 & 6 & 0 \end{matrix}\right)$$

如果前向传播是采用的MEAN操作，则将每个误差求平均，unsample后的矩阵为：

$$\left( \begin{matrix}0.5 & 0.5 & 2 & 2 \\ 0.5 & 0.5 & 2 & 2 \\ 1 & 1 & 1.5 & 1.5 \\ 1 & 1 & 1.5 & 1.5 \end{matrix}\right)$$

完成usample操作后，可得到推导前一层误差的公式为：

$$\begin{equation} \boldsymbol{\delta}^{l-1} = unsample(\boldsymbol{\delta}^{l}) \odot \sigma'(\boldsymbol{z}^{l-1}) \end{equation}$$

这和全连接网络的反向推导公式类似：

$$\begin{equation} \boldsymbol{\delta}^l = (\boldsymbol{w}^{l+1})^T\boldsymbol{\delta}^{l+1} \odot \sigma'(\boldsymbol{z}^l) \end{equation}$$

可以看到，只有第一项不同。


### 7.2.3 已知卷积层的误差，反向推导上一隐藏层的误差

在前向传播时，假设卷积层输入图像大小为$n\times n$，过滤器大小为$f\times f$，则输出图像大小为$(n-f+1)\times (n-f+1)$。同样地，我们也反过来，要从缩小后区域的误差，还原上一层较大区域的误差。

首先要进行padding操作，即沿着误差矩阵边缘用0进行填充。

假设卷积层$l$的误差矩阵$$\boldsymbol{\delta}^l$$大小为$(n-f+1)\times (n-f+1)$，则padding后大小为$(n+f-1)\times (n+f-1)$。

然后对权值矩阵$\boldsymbol{w}^l$进行旋转180度的操作，即上下翻转一次，左右再翻转一次。

最后得到的推导公式如下：

$$\begin{equation} \boldsymbol{\delta}^{l-1} = padding(\boldsymbol{\delta}^{l}) * rot180(\boldsymbol{w}^{l}) \odot \sigma'(\boldsymbol{z}^{l-1}) \end{equation}$$


### 7.2.4 已知卷积层的误差，推导该层的权值和偏置的梯度

经过以上各步骤，我们已经算出每一层的误差了，那么：

1）对于全连接层，可以按照普通网络的反向传播算法求该层的权值和偏置梯度；

2）对于池化层，它并没有权值和偏置，也不用求梯度；

3）只有卷积层的权值和偏置需要求解。

对于卷积层的权值，计算公式为：

$$\frac{\partial C}{\partial\boldsymbol{w}^l} = rot180(\boldsymbol{a}^{l-1}) * \boldsymbol{\delta}^{l}$$

对比一下全连接网络的公式：

$$ \frac{\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l $$

主要的区别在于，要对前一层的输出做翻转180度的操作。

而对于偏置$\boldsymbol{b}$，则要特殊一点。因为误差$\boldsymbol{\delta}$是三维张量（tensor），而$\boldsymbol{b}$只是一个向量（vector），不能向全连接网络中那样直接令其梯度和误差相等。通常的做法是将误差$\boldsymbol{\delta}$的各个子矩阵的项分别求和，得到一个误差向量，即为$\boldsymbol{b}$的梯度：

$$ \frac{\partial C}{\partial\boldsymbol{b}^l} = \sum_{u,v}(\boldsymbol{\delta}^l)_{u,v} $$




# 循环神经网络（Recurrent Neural Networks，RNN）

# 深度置信网络（Deep Belief Networks，DBN）





