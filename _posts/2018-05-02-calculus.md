---
layout: post
title: "微积分基础" 
---
# 1、序列与极限

A sequence of real numbers is function whose domain is the set of natural numbers $1,2,...,k,...$ and whose range is contained in $\mathbb{R}$. Thus, a sequence of real numbers can be viewed as a set of numbers $\\{x_1,x_2,...,x_k,...\\}$, which is often denoted as $$\{x_k\}$$.

A sequence $$\{x_k\}$$ is increasing if $x_1 < x_2 < \cdots < x_k \cdots$; that is, $$x_k < x_{k+1}$$ for all $k$. If $$x_k \leq x_{k+1}$$, then we say that the sequence is nondecreasing. Similarly, we can define decreasing and nonincreasing sequences. Nonincreasing or nondecreasing sequences are called monotone sequences.

A number $x^* \in \mathbb{R}$ is called the limit of the sequence $$\\{x_k\\}$$ if for any positive $\epsilon$
 there is a number $K$ (which may depend on $\epsilon$) such that for all $k > K$, $$|x_k - x^*| < \epsilon$$. In this case we write

$$x^* = \lim_{k \rightarrow \infty}x_k$$

or

$$x_k \rightarrow x^*$$

A sequence that has a limit is called a convergent sequence.

The notion of a sequence can be extended to sequences with elements in $\mathbb{R}^n$. Specifically, a sequence in $\mathbb{R}^n$ is a function whose domain is the set of natural numbers $1,2,...,k,...$ and whose range is contained in $\mathbb{R}^n$. We use the notation $\\{\mathbf{x}^{(k)}\\}$ for sequences in $\mathbb{R}^n$. For limits of sequences in $\mathbb{R}^n$, we need to replace absolute values with vector norms. 

$\textbf{Theorem: }$ A convergent sequence has only one limit.

A sequence $\\{\mathbf{x}^{(k)}\\}$ in $\mathbb{R}^n$ is bounded if there exists a number $B \geq 0$ such that $\\|\mathbf{x}^{(k)}\\| \leq B$ for all $k = 1,2,...$.

$\textbf{Theorem: }$ Every convergent sequence is bounded.

Suppose that we are given a sequence $\\{\mathbf{x}^{(k)}\\}$ and an increasing sequence of natural numbers $$\{m_k\}$$. The sequence

$$\\{\mathbf{x}^{(m_k)}\\} = \{\mathbf{x}^{(m_1)},\mathbf{x}^{(m_2)},...\}$$

is called a subsequence of the sequence $\\{\mathbf{x}^{(k)}\\}$. A subsequence of a given sequence can thus be obtained by neglecting some elements of the given sequence.

$\textbf{Theorem: Bolzano-Weierstrass Theorem.}$ Consider a convergent sequence $\\{\mathbf{x}^{(k)}\\}$ with limit $$\mathbf{x}^*$$. Then, any subsequence of $\\{\mathbf{x}^{(k)}\\}$ also converges to $\mathbf{x}^*$.

A function $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is __continuous__ at $$\mathbf{x}_0$$ if for all $\epsilon > 0$, there exists $\delta > 0$ such that $\\|\mathbf{x} - \mathbf{x}_0\\| < \delta \Rightarrow \\|\mathbf{f}(\mathbf{x}) - \mathbf{f}(\mathbf{x}_0)\\| < \epsilon$. If the function $\mathbf{f}$ is continuous at every point in $\mathbb{R}^n$, we say that it is continuous on $\mathbb{R}^n$. Note that $\mathbf{f} = [f_1,f_2,...,f_m]^T$ is continuous if and only if each component $f_i (i=1,2,...,m)$ is continuous.

Consider a function $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and a point $$\mathbf{x}_0 \in \mathbb{R}^n$$. Suppose that there exists $\mathbf{f}^*$ such that for any convergent sequence $\\{\mathbf{x}^{(k)}\\}$ with limit $$\mathbf{x}_0$$, we have

$$\lim_{k \rightarrow \infty}\mathbf{f}(\mathbf{x}^{(k)}) = \mathbf{f}^*$$

Then, we use the notation

$$\lim_{\mathbf{x} \rightarrow \mathbf{x}_0}\mathbf{f}(\mathbf{x})$$

to represent the limit $\mathbf{f}^*$.

It turns out that $\mathbf{f}$ is continuous at $$\mathbf{x}_0$$ if and only if for any convergent sequence $\\{\mathbf{x}^{(k)}\\}$ with limit $\mathbf{x}_0$, we have 

$$\lim_{k \rightarrow \infty}\mathbf{f}(\mathbf{x}^{(k)}) = \mathbf{f}(\lim_{k \rightarrow \infty}\mathbf{x}^{(k)}) = \mathbf{f}(\mathbf{x}_0)$$

Therefore, using the notation introduced above, the function $\mathbf{f}$ is continuous at $$\mathbf{x}_0$$ if and only if

$$\lim_{\mathbf{x} \rightarrow \mathbf{x}_0}\mathbf{f}(\mathbf{x}) = \mathbf{f}(\mathbf{x}_0)$$

We end this section with some results involving sequences and limits of matrices. 

We say that a sequence $$\{\mathbf{A}_k\}$$ of $m \times n$ matrices converges to the $m \times n$ matrix $\mathbf{A}$ if

$$\lim_{k \rightarrow \infty} \|\mathbf{A}_k - \mathbf{A}\| = 0$$

$\textbf{Lemma: }$ Let $\mathbf{A} \in \mathbb{R}^{n \times n}$. Then, $\lim_{k \rightarrow \infty}\mathbf{A}^k = \mathbf{O}$ if and only if the eigenvalues of $\mathbf{A}$ satisfy $\|\lambda_i(\mathbf{A})\| < 1 (i = 1,2,...,n)$.

$\textbf{Lemma: }$ The series of $n \times n$ matrices

$$\mathbf{I}_n + \mathbf{A} + \mathbf{A}^2 + \cdots + \mathbf{A}^k + \cdots$$ 

converges if and only if $\lim_{k \rightarrow \infty}\mathbf{A}^k = \mathbf{O}$. In this case the result of the series equals $(\mathbf{I}_n - \mathbf{A})^{-1}$.

A matrix-valued function $\mathbf{A}: \mathbb{R}^r \rightarrow \mathbb{R}^{n \times n}$ is continuous at a point $\mathbf{x}_0 \in \mathbb{R}^r$ if

$$\lim_{\|\mathbf{x}-\mathbf{x}_0\| \rightarrow 0} \|\mathbf{A}(\mathbf{x}) - \mathbf{A}(\mathbf{x}_0)\| = 0$$

$\textbf{Lemma: }$ Let $\mathbf{A}: \mathbb{R}^r \rightarrow \mathbb{R}^{n \times n}$ be an $n \times n$ matrix-valued function that is continuous at $\mathbf{x}_0$. If $\mathbf{A}(\mathbf{x}_0)^{-1}$ exists, then $\mathbf{A}(\mathbf{x})^{-1}$ exists for $\mathbf{x}$ sufficiently close to $\mathbf{x}_0$ and $\mathbf{A}(\cdot)^{-1}$ is continuous at $\mathbf{x}_0$.


# 2、可微性

Differential calculus is based on the idea of approximating an arbitrary function by an __affine function__. A function $\mathcal{A}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is __affine__ if there exists a linear function $\mathcal{L}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and a vector $\mathbf{y} \in \mathbb{R}^m$ such that

$$\mathcal{A}(\mathbf{x}) = \mathcal{L}(\mathbf{x}) + \mathbf{y}$$

for every $\mathbf{x} \in \mathbb{R}^n$. 

Consider a function  $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and a point $\mathbf{x}_0 \in \mathbb{R}^n$. We wish to find an affine function $\mathcal{A}$ that approximates $\mathbf{f}$ near the point $\mathbf{x}_0$.

First, it is natrual to impose the condition

$$\mathcal{A}(\mathbf{x}_0) = \mathbf{f}(\mathbf{x}_0)$$

Hence, we obtain

$$\mathcal{A}(\mathbf{x}) = \mathcal{L}(\mathbf{x} - \mathbf{x}_0) + \mathbf{f}(\mathbf{x}_0)$$

Next, we require that $\mathcal{A}(\mathbf{x})$ approaches $\mathbf{f}(\mathbf{x})$ faster than $\mathbf{x}$ approaches $\mathbf{x}_0$; that is,

$$\lim_{\mathbf{x}\rightarrow \mathbf{x}_0} \frac{\|\mathbf{f}(\mathbf{x})-\mathcal{A}(\mathbf{x})\|}{\|\mathbf{x}-\mathbf{x}_0\|} = 0$$

The conditions above on $\mathcal{A}$ ensure that $\mathcal{A}$ approximates $\mathbf{f}$ near $\mathbf{x}_0$ in the sense that the error in the approximation at a given point is "very small" compared with the distance of the point from $\mathbf{x}_0$.

In summary, a function $\mathbf{f}: \Omega \rightarrow \mathbb{R}^m, \Omega \subset \mathbb{R}^n$, is said to be __differentiable__ at $$\mathbf{x}_0 \in \Omega$$ if there is an affine function that approximates $\mathbf{f}$ near $$\mathbf{x}_0$$; that is, there exists a linear function $\mathcal{L}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ such that

$$\lim_{\mathbf{x}\rightarrow \mathbf{x}_0, \mathbf{x} \in \Omega} \frac{\|\mathbf{f}(\mathbf{x})-(\mathcal{L}(\mathbf{x} - \mathbf{x}_0) + \mathbf{f}(\mathbf{x}_0))\|}{\|\mathbf{x}-\mathbf{x}_0\|} = 0$$

The linear function $\mathcal{L}$ above is determined __uniquely__ by $\mathbf{f}$ and $\mathbf{x}_0$ and is called the derivative of $\mathbf{f}$ at $\mathbf{x}_0$. The function $\mathbf{f}$ is said to be differentiable on $\Omega$ if $\mathbf{f}$ is differentiable at every point of its domain $\Omega$.


# 3、导数矩阵

Any linear transformation from $\mathbb{R}^n$ to $\mathbb{R}^m$, and in particular the derivative $\mathcal{L}$ of $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$, can be represeted by an $m \times n$ matrix. To find the matrix representation $\mathbf{L}$ of the derivative $\mathcal{L}$, we use the natural basis $$\{\mathbf{e}_1,\mathbf{e}_2,...,\mathbf{e}_n\}$$ for $\mathbb{R}^n$. Consider the vectors

$$\mathbf{x}_j = \mathbf{x}_0 + t\mathbf{e}_j (j = 1,2,...,n)$$

By the definition of the derivative, we have 

$$\lim_{t \rightarrow 0} \frac{\mathbf{f}(\mathbf{x}_j)-(t\mathbf{L}\mathbf{e}_j + \mathbf{f}(\mathbf{x}_0))}{t} = \mathbf{0}$$

for $j=1,2,...,n$. This means that

$$\lim_{t \rightarrow 0} \frac{\mathbf{f}(\mathbf{x}_j)-\mathbf{f}(\mathbf{x}_0)}{t} = \mathbf{L}\mathbf{e}_j$$

for $j=1,2,...,n$. But $\mathbf{L}\mathbf{e}_j$ is the $j$th column of the matrix $\mathbf{L}$. On the other hand, the vector $\mathbf{x}_j$ differs from $\mathbf{x}_0$ only in the $j$th coordinate, and in that coordinate the difference is just the number $t$. Therefore, the left side of the preceding equation is the partial derivative

$$\frac{\partial\mathbf{f}}{\partial x_j}(\mathbf{x}_0)$$

Because vector limits are computed by taking the limit of each of each coordinate function, it follows that if 

$$\mathbf{f}(\mathbf{x}) = \begin{bmatrix} f_1(\mathbf{x}) \\ f_2(\mathbf{x}) \\ \vdots \\ f_m(\mathbf{x}) \end{bmatrix}$$

then

$$\frac{\partial\mathbf{f}}{\partial x_j}(\mathbf{x}_0) = \begin{bmatrix} \frac{\partial f_1}{\partial x_j}(\mathbf{x}_0) \\ \frac{\partial f_2}{\partial x_j}(\mathbf{x}_0) \\ \vdots \\ \frac{\partial f_m}{\partial x_j}(\mathbf{x}_0)\end{bmatrix}$$

and the matrix $\mathbf{L}$ has the form

$$D\mathbf{f}(\mathbf{x}_0) = \left[\frac{\partial\mathbf{f}}{\partial x_1}(\mathbf{x}_0) \cdots \frac{\partial\mathbf{f}}{\partial x_n}(\mathbf{x}_0)\right] 
= \left[
\begin{array}
&\frac{\partial f_1}{\partial x_1}(\mathbf{x}_0) & \cdots & \frac{\partial f_1}{\partial x_n}(\mathbf{x}_0) \\
\vdots & & \vdots \\
\frac{\partial f_m}{\partial x_1}(\mathbf{x}_0) & \cdots & \frac{\partial f_m}{\partial x_n}(\mathbf{x}_0)
\end{array}
\right]$$

The matrix $\mathbf{L}$ is called the __jacobian matrix__, or derivative matrix, of $\mathbf{f}$ at $\mathbf{x}_0$, and is denoted $D\mathbf{f}(\mathbf{x}_0)$. For convenience, we often refer to $D\mathbf{f}(\mathbf{x}_0)$ simply as the derivative of $\mathbf{f}$ at $\mathbf{x}_0$.

$\textbf{Theorem: }$ If a function $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable at $\mathbf{x}_0$, then the derivative of $\mathbf{f}$ at $\mathbf{x}_0$ is determined uniquely and is represented by the $m \times n$ derivative matrix $D\mathbf{f}(\mathbf{x}_0)$. The affine approximation to $\mathbf{f}$ near $\mathbf{x}_0$ is then given by

$$\mathcal{A}(\mathbf{x}) = \mathbf{f}(\mathbf{x}_0) + D\mathbf{f}(\mathbf{x}_0)(\mathbf{x} - \mathbf{x}_0) $$

in the sense that

$$\mathbf{f}(\mathbf{x}) = \mathcal{A}(\mathbf{x}) + \mathbf{r}(\mathbf{x})$$

and $$\lim_{\mathbf{x}\rightarrow \mathbf{x}_0}\|\mathbf{r}(\mathbf{x})\|/\|\mathbf{x} - \mathbf{x}_0\| = 0$$. The columns of the derivative matrix $D\mathbf{f}(\mathbf{x}_0)$ are vector partial derivatives. The vector

$$\frac{\partial\mathbf{f}}{\partial x_j}(\mathbf{x}_0)$$

is a tangent vector at $\mathbf{x}_0$ to the curve $\mathbf{f}$ obtained by varying only the $j$th coordinate of $\mathbf{x}$.

If $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable, then the function $\nabla f$ defined by

$$\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1}(\mathbf{x}) \\ \vdots \\ \frac{\partial f}{\partial x_n}(\mathbf{x})\end{bmatrix} = (D f(\mathbf{x}))^T$$

is called the __gradient__ of $f$. 

Given $f: \mathbb{R}^n \rightarrow \mathbb{R}$, if $\nabla f$ is differentiable, we say that $f$ is twice differentiable, and we write the derivative of $\nabla f$ as

$$
D^2f = \left[
\begin{array}
&\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_2\partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n\partial x_1}\\ 
\frac{\partial^2 f}{\partial x_1\partial x_2} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_n\partial x_2}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial^2 f}{\partial x_1\partial x_n} & \frac{\partial^2 f}{\partial x_2\partial x_n} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{array}
\right]
$$

The notation $\frac{\partial^2 f}{\partial x_j\partial x_i}$ represents taking the partial derivative of $f$ with respect to $x_i$ first, then with respect to $x_j$. The matrix $D^2f(\mathbf{x})$ is called the __Hessian matrix__ of $f$ at $\mathbf{x}$.

A function $\mathbf{f}: \Omega \rightarrow \mathbb{R}^m, \Omega \subset \mathbb{R}^n$, is said to be __continuously differentiable__ on $\Omega$ if it is differentiable on $\Omega$, and $D\mathbf{f}: \Omega \rightarrow \mathbb{R}^{m \times n}$ is continuous. In this case, we write $\mathbf{f} \in \mathcal{C}^1$. If $\mathbf{f}$ has continuous partial derivatives of order $p$, then we write $\mathbf{f} \in \mathcal{C}^p$.

Note that the Hessian matrix is a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ at $\mathbf{x}$ is symmetric if $f$ is twice continuously differentiable at $\mathbf{x}$. This is well-known result from calculus called Clairaut's theorem or Schwarz's theorem. However, if the second partial derivatives of $f$ are not continuous, then there is no guarantee that the Hessian is symmetric.

# 4、微分法则

We now introduce the __chain rule__ for differentiating the composition $g(\mathbf{f}(t))$, of a function $\mathbf{f}: \mathbb{R} \rightarrow \mathbb{R}^n$ and a function $g: \mathbb{R}^n \rightarrow \mathbb{R}$.

$\textbf{Theorem: }$ Let $g: \mathcal{D} \rightarrow \mathbb{R}$ be differentiable on an open set $\mathcal{D} \subset \mathbb{R}^n$, and let $\mathbf{f}: (a,b) \rightarrow \mathcal{D}$ be differentiable on $(a,b)$. Then, the composite function $h: (a,b) \rightarrow \mathbb{R}$ given by $h(t) = g(\mathbf{f}(t))$ is differentiable on $(a,b)$, and 

$$h'(t) = Dg(\mathbf{f}(t))D\mathbf{f}(t) = \nabla g(\mathbf{f}(t))^T \begin{bmatrix} f'_1(t) \\ \vdots \\ f'_n(t)\end{bmatrix}$$

Next, we present the __product rule__. Let $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and $\mathbf{g}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be two differentiable functions. Define the function $h: \mathbb{R}^n \rightarrow \mathbb{R}$ by $h(\mathbf{x} = \mathbf{f}(\mathbf{x})^T\mathbf{g}(\mathbf{x}))$. Then, h is also differentiable and

$$Dh(\mathbf{x}) = \mathbf{f}(\mathbf{x})^TD\mathbf{g}(\mathbf{x}) + \mathbf{g}(\mathbf{x})^TD\mathbf{f}(\mathbf{x})$$

We end this section with a list of some useful furmulas from multivariable calculus. In each case, we compute the derivative with respect to $\mathbf{x}$. Let $\mathbf{A} \in \mathbb{R}{m \times n}$ be a given matrix and $\mathbf{y} \in \mathbb{R}^m$ a given vector. Then,

$$D(\mathbf{y}^T\mathbf{A}\mathbf{x}) = \mathbf{y}^T\mathbf{A}$$

$$D(\mathbf{x}^T\mathbf{A}\mathbf{x}) = \mathbf{x}^T(\mathbf{A} + \mathbf{A}^T) (\text{if } m=n)$$

# 5、水平集与梯度

The level set of a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ at level $c$ is the set of points

$$S = \{\mathbf{x} | f(\mathbf{x}) = c\}$$

To say that a point $\mathbf{x}_0$ is on the level set $S$ at level $c$ means that $f(\mathbf{x}_0) = c$. 

$\textbf{Theorem: }$ The vector $\nabla f(\mathbf{x}_0)$ is orthogonal to the tangent vector to an arbitrary smooth curve passing through $\mathbf{x}_0$ on the level set determined by $f(\mathbf{x}) = f(\mathbf{x}_0)$.

Because $\nabla f(\mathbf{x}_0)$ is orthogonal to the level set through $\mathbf{x}_0$ determined by $f(\mathbf{x}) = f(\mathbf{x}_0)$, we deduce the following fact: $\nabla f(\mathbf{x}_0)$ is the direction of maximum rate of increace of $f$ at $\mathbf{x}_0$.

The __graph__ of $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is the set $\\{[\mathbf{x}^T, f(\mathbf{x})]^T \| \mathbf{x} \in \mathbb{R}^n\\} \subset \mathbb{R}^{n+1}$. The notion of the gradient of a function has an alternative useful interpretation in terms of the tangent hyperplane to its graph. 

To proceed, let $$\mathbf{x}_0 \in \mathbb{R}^n$$ and $$y_0 = f(\mathbf{x}_0)$$. If $f$ is differentiable at $$\mathbf{x}_0$$, then the graph admits a nonvertical tangent hyperplane at $$\mathbf{z} = [\mathbf{x}_0^T, y_0]$$. The hyperplane through $\mathbf{z}$ is the set of all points $$[x_1,...,x_n,y]^T \in \mathbb{R}^{n+1}$$ satisfying the equation

$$u_1(x_1 - \mathbf{x}_{01}) + \cdots + u_n(x_n - \mathbf{x}_{0n}) + v(y - y_0) = 0$$

where the vector $$[u_1,...,u_n,v]^T \in \mathbb{R}^{n+1}$$ is normal to the hyperplane. Assuming that this hyperplane is nonvertical (that is, $v \neq 0$), let

$$d_i = -\frac{u_i}{v}$$

Thus, we can rewrite the hyperplane equation above as

$$y = d_1(x_1 - \mathbf{x}_{01}) + \cdots + d_n(x_n - \mathbf{x}_{0n}) + y_0$$

We can think of the right side of the above equation as a function $y: \mathbb{R}^n \rightarrow \mathbb{R}$. Observe that for the hyperplane to be tangent to the graph of $f$, the functions $f$ and $y$ must have the same partial derivatives at the point $\mathbf{x}_0$. Thus

$$\nabla f(\mathbf{x}_0) = \begin{bmatrix} d_1 \\ \vdots \\d_n\end{bmatrix}$$

Hence, if $f$ is differentialbe at $\mathbf{x}_0$, its tangent hyperplane can be written in terms of its gradient, as given by the equation

$$y - y_0 = (\mathbf{x}-\mathbf{x}_0)^T\nabla f(\mathbf{x}_0)$$


# 6、大$O$和小$o$记法

We introduce the __order symbols__, $O$ and $o$.

Let $g$ be a real-valued function defined in some neighborhood of $\mathbf{0} \in \mathbb{R}^n$, with $g(\mathbf{x}) \neq 0$ if $\mathbf{x} \neq \mathbf{0}$. Let $\mathbf{f}: \Omega \rightarrow \mathbb{R}^m$ be defined in a domain $\Omega \subset \mathbb{R}^n$ that includes $\mathbf{0}$. Then, we write

1) $\mathbf{f}(\mathbf{x}) = O(g(\mathbf{x}))$ to mean that the quotient $\\|\mathbf{f}(\mathbf{x})\\|/\|g(\mathbf{x})\|$ in bounded near $\mathbf{0}$; that is, there exist numbers $K > 0$ and $\delta > 0$ such that if $\\|\mathbf{x}\\| < \delta (\mathbf{x} \in \Omega)$, then $\\|\mathbf{f}(\mathbf{x})\\|/\|g(\mathbf{x})\| \leq K$.

2) $\mathbf{f}(\mathbf{x}) = o(g(\mathbf{x}))$ to mean that

$$\lim_{\mathbf{x}\rightarrow\mathbf{0},\mathbf{x}\in\Omega}\frac{\|\mathbf{f}(\mathbf{x})\|}{|g(\mathbf{x})|} = 0$$

Note that if $\mathbf{f}(\mathbf{x}) = o(g(\mathbf{x}))$, then $\mathbf{f}(\mathbf{x}) = O(g(\mathbf{x}))$ (but the converse is not necessarily true). Also, if $\mathbf{f}(\mathbf{x}) = O(\\|\mathbf{x}\\|^p)$, then $\mathbf{f}(\mathbf{x}) = o(\\|\mathbf{x}\\|^{p-\epsilon})$ for any $\epsilon > 0$.


# 7、泰勒级数和中值定理

The basis for many numerical methods and models for optimization is Taylor's furmula, which is given by Taylor's theorem.

$\textbf{Theorem: Taylor's Theorem.}$ Assume that a function $f: \mathbb{R} \rightarrow \mathbb{R}$ is $m$ times continuously differentiable (i.e., $f \in \mathcal{C}^m$) on an interval $[a,b]$. Denote $h = b - a$. Then,

$$f(b) = f(a) + \frac{h}{1!}f^{(1)}(a) + \frac{h^2}{2!}f^{(2)}(a) + \cdots + \frac{h^{m-1}}{(m-1)!}f^{(m-1)}(a) + R_m$$

(called Taylor's formula) where $f^{(i)}$ is the $i$th derivative of $f$, and

$$R_m = \frac{h^m}{m!}f^{(m)}(a+\theta h)$$

with $\theta \in (0,1)$.

By the continuity of $f^{(m)}$, we have $f^{(m)}(a+\theta h) \rightarrow f^{(m)}(a)$ as $h \rightarrow 0$; that is, $f^{(m)}(a+\theta h) = f^{(m)}(a) + o(1)$. Therefore,

$$\frac{h^m}{m!}f^{(m)}(a+\theta h) = \frac{h^m}{m!}f^{(m)}(a) + o(h^m)$$

since $h^mo(1) = o(h^m)$. We may then write Taylor's formula as

$$f(b) = f(a) + \frac{h}{1!}f^{(1)}(a) + \frac{h^2}{2!}f^{(2)}(a) + \cdots + \frac{h^{m}}{m!}f^{(m)}(a) + o(h^m)$$

If, in addition, we assume that $f \in \mathcal{C}^{m+1}$, we may replace the term $o(h^m)$ above by $O(h^{m+1})$. To see this, we first write Taylor's formula with $$R_{m+1}$$:

$$f(b) = f(a) + \frac{h}{1!}f^{(1)}(a) + \frac{h^2}{2!}f^{(2)}(a) + \cdots + \frac{h^{m}}{m!}f^{(m)}(a) + R_{m+1}$$

where

$$R_{m+1} = \frac{h^{m+1}}{(m+1)!}f^{(m+1)}(a+\theta' h)$$

with $\theta' \in (0,1)$. Because $f^{(m+1)}$ is bounded on $[a,b]$,

$$R_{m+1} = O(h^{m+1})$$

Therefore, if $f \in \mathcal{C}^{m+1}$, we may write Taylor's formula as

$$f(b) = f(a) + \frac{h}{1!}f^{(1)}(a) + \frac{h^2}{2!}f^{(2)}(a) + \cdots + \frac{h^{m}}{m!}f^{(m)}(a) + O(h^{m+1})$$

We now turn to the Taylor series expansion of a real-valued function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ about the point $\mathbf{x}_0 \in \mathbb{R}^n$.

If $f \in \mathcal{C}^2$, we can obtain

$$f(\mathbf{x}) = f(\mathbf{x_0}) + \frac{1}{1!}Df(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0) + \frac{1}{2!}(\mathbf{x}-\mathbf{x}_0)^TD^2f(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0) + o(\|\mathbf{x}-\mathbf{x}_0\|^2)$$

If we assume that $f \in \mathcal{C}^3$, we can obtain

$$f(\mathbf{x}) = f(\mathbf{x_0}) + \frac{1}{1!}Df(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0) + \frac{1}{2!}(\mathbf{x}-\mathbf{x}_0)^TD^2f(\mathbf{x}_0)(\mathbf{x}-\mathbf{x}_0) + O(\|\mathbf{x}-\mathbf{x}_0\|^3)$$


We end this a statement of the mean value theorem, which is closely related to Taylor's theorem.

$\textbf{Theorem: Mean Value Theorem.}$ If a function $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable on an open set $\Omega \subset \mathbb{R}^n$, then for any pair of points $\mathbf{x},\mathbf{y} \in \Omega$, there exists a $m\times n$ matrix $\mathbf{M}$ such that

$$\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{y}) = \mathbf{M}(\mathbf{x}-\mathbf{y})$$

The mean value theorem follows from Taylor's Theorem applied to each component of $\mathbf{f}$. It is easy to see that $\mathbf{M}$ is a matrix whose rows are the rows of $D\mathbf{f}$ evaluated at points that lie on the line segment joining $\mathbf{x}$ and $\mathbf{y}$ (these points may differ from row to row).

